# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer((16384, 16384), "float16"), B: T.Buffer((16384, 8192), "int8"), C: T.Buffer((16384, 16384), "float16")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        B_decompress = T.alloc_buffer((16384, 16384), "float16")
        for i, j in T.grid(16384, 16384):
            with T.block("B_decompress"):
                vi, vj = T.axis.remap("SS", [i, j])
                T.reads(B[vi, vj // 2:vj // 2 + 2])
                T.writes(B_decompress[vi, vj])
                B_decompress[vi, vj] = T.Select(vj % 32 * 4 % 8 <= 5, T.Cast("float16", T.bitwise_and(T.shift_right(T.Cast("int32", B[vi, vj // 32 * 16 + vj % 32 * 4 // 8]), vj % 32 * 4 % 8), 15)), T.Cast("float16", T.bitwise_or(T.Cast("int8", T.bitwise_and(T.shift_right(T.Cast("int32", B[vi, vj // 32 * 16 + vj % 32 * 4 // 8]), vj % 32 * 4 % 8), T.shift_left(1, 8 - vj % 32 * 4 % 8) - 1)), T.Cast("int8", T.bitwise_and(T.bitwise_and(T.shift_left(T.Cast("int32", B[vi, vj // 32 * 16 + vj % 32 * 4 // 8 + 1]), 8 - vj % 32 * 4 % 8), T.shift_left(15, 8 - vj % 32 * 4 % 8)), 15)))))
        for i, j, k in T.grid(16384, 16384, 16384):
            with T.block("B"):
                vi, vj, vk = T.axis.remap("SSR", [i, j, k])
                T.reads(A[vi, vk], B_decompress[vj, vk])
                T.writes(C[vi, vj])
                with T.init():
                    C[vi, vj] = T.float16(0)
                C[vi, vj] = C[vi, vj] + A[vi, vk] * B_decompress[vj, vk]
2023-05-02 14:12:59 [INFO] Logging directory: ./logs/gemm_M16384N16384K16384_4b/logs
2023-05-02 14:12:59 [INFO] LocalBuilder: max_workers = 32
2023-05-02 14:12:59 [INFO] LocalRunner: max_workers = 1
2023-05-02 14:13:12 [INFO] [task_scheduler.cc:159] Initializing Task #0: "main"
2023-05-02 14:13:12 [INFO] [task_scheduler.cc:260] Task #0 has finished. Remaining task(s): 0
2023-05-02 14:13:12 [INFO] [task_scheduler.cc:320] 
 ID | Name |          FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done 
------------------------------------------------------------------------------------------------------------
  0 | main | 8800387989504 |      1 |            N/A |          N/A |                   N/A |      0 |    Y 
------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer((16384, 16384), "float16"), B: T.Buffer((16384, 8192), "int8"), C: T.Buffer((16384, 16384), "float16")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        C_reindex_shared_dyn = T.alloc_buffer((256, 256, 4, 4, 16, 16), "float16", scope="shared.dyn")
        C_reindex_shared_dyn_wmma_accumulator = T.alloc_buffer((256, 256, 4, 4, 16, 16), "float16", scope="wmma.accumulator")
        A_reindex_shared_dyn = T.alloc_buffer((16384, 16384), "float16", scope="shared.dyn")
        B_decompress_reindex_shared_dyn = T.alloc_buffer((16384, 16384), "float16", scope="shared.dyn")
        A_reindex_shared_dyn_wmma_matrix_a = T.alloc_buffer((16384, 16384), "float16", scope="wmma.matrix_a")
        B_decompress_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((16384, 16384), "float16", scope="wmma.matrix_b")
        for ax0_0_0_ax1_0_0_fused in T.thread_binding(1024, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for ax0_0_1_ax1_0_1_fused in T.thread_binding(8, thread="blockIdx.x"):
                for ax0_0_2_ax1_0_2_fused in T.thread_binding(8, thread="threadIdx.y"):
                    for ax0_0_3_init, ax1_0_3_init, ax0_0_4_init, ax1_0_4_init in T.grid(2, 2, 2, 2):
                        with T.block("B_o_init"):
                            v0_o = T.axis.spatial(1024, ax0_0_0_ax1_0_0_fused // 64 * 64 + ax0_0_1_ax1_0_1_fused // 2 * 16 + ax0_0_2_ax1_0_2_fused // 2 * 4 + ax0_0_3_init * 2 + ax0_0_4_init)
                            v1_o = T.axis.spatial(1024, ax0_0_0_ax1_0_0_fused % 64 * 16 + ax0_0_1_ax1_0_1_fused % 2 * 8 + ax0_0_2_ax1_0_2_fused % 2 * 4 + ax1_0_3_init * 2 + ax1_0_4_init)
                            T.reads()
                            T.writes(C_reindex_shared_dyn_wmma_accumulator[v0_o // 4, v1_o // 4, v0_o % 4, v1_o % 4, 0:16, 0:16])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "warp_execution": T.int64(1)})
                            C_1 = T.match_buffer(C_reindex_shared_dyn_wmma_accumulator[v0_o // 4, v1_o // 4, v0_o % 4, v1_o % 4, 0:16, 0:16], (16, 16), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                            T.tvm_fill_fragment(C_1.data, 16, 16, 16, C_1.elem_offset // C_1.strides[0] // 16 * (C_1.strides[0] // 16) + C_1.elem_offset % C_1.strides[0] // 16, T.float32(0))
                    for ax2_0_0 in range(512):
                        for ax0_ax1_fused_0 in range(8):
                            for ax0_ax1_fused_1 in T.thread_binding(8, thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(4):
                                        with T.block("A_reindex_shared.dyn"):
                                            v0 = T.axis.spatial(16384, ax0_0_0_ax1_0_0_fused // 64 * 1024 + ax0_0_1_ax1_0_1_fused // 2 * 256 + (ax0_ax1_fused_0 * 1024 + ax0_ax1_fused_1 * 128 + ax0_ax1_fused_2 * 4 + ax0_ax1_fused_3) // 32)
                                            v1 = T.axis.spatial(16384, ax2_0_0 * 32 + (ax0_ax1_fused_0 * 1024 + ax0_ax1_fused_1 * 128 + ax0_ax1_fused_2 * 4 + ax0_ax1_fused_3) % 32)
                                            T.reads(A[v0, v1])
                                            T.writes(A_reindex_shared_dyn[v0, v1])
                                            T.block_attr({"buffer_dim_align": [[0, 0, 32, 8]]})
                                            A_reindex_shared_dyn[v0, v1] = A[v0, v1]
                        for ax0_ax1_fused_0 in range(8):
                            for ax0_ax1_fused_1 in T.thread_binding(8, thread="threadIdx.y"):
                                for ax0_ax1_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_fused_3 in T.vectorized(2):
                                        with T.block("B_decompress_reindex_shared.dyn"):
                                            v0 = T.axis.spatial(16384, ax0_0_0_ax1_0_0_fused % 64 * 256 + ax0_0_1_ax1_0_1_fused % 2 * 128 + (ax0_ax1_fused_0 * 512 + ax0_ax1_fused_1 * 64 + ax0_ax1_fused_2 * 2 + ax0_ax1_fused_3) // 32)
                                            v1 = T.axis.spatial(16384, ax2_0_0 * 32 + (ax0_ax1_fused_0 * 512 + ax0_ax1_fused_1 * 64 + ax0_ax1_fused_2 * 2 + ax0_ax1_fused_3) % 32)
                                            T.reads(B[v0, v1 // 2:v1 // 2 + 2])
                                            T.writes(B_decompress_reindex_shared_dyn[v0, v1])
                                            T.block_attr({"buffer_dim_align": [[0, 0, 32, 8]]})
                                            B_decompress_reindex_shared_dyn[v0, v1] = T.Select(v1 % 32 * 4 % 8 <= 5, T.Cast("float16", T.bitwise_and(T.shift_right(T.Cast("int32", B[v0, v1 // 32 * 16 + v1 % 32 * 4 // 8]), v1 % 32 * 4 % 8), 15)), T.Cast("float16", T.bitwise_or(T.Cast("int8", T.bitwise_and(T.shift_right(T.Cast("int32", B[v0, v1 // 32 * 16 + v1 % 32 * 4 // 8]), v1 % 32 * 4 % 8), T.shift_left(1, 8 - v1 % 32 * 4 % 8) - 1)), T.Cast("int8", T.bitwise_and(T.bitwise_and(T.shift_left(T.Cast("int32", B[v0, v1 // 32 * 16 + v1 % 32 * 4 // 8 + 1]), 8 - v1 % 32 * 4 % 8), T.shift_left(15, 8 - v1 % 32 * 4 % 8)), 15)))))
                        for ax2_0_1 in range(1):
                            for ax0_0, ax1_0 in T.grid(4, 2):
                                with T.block("A_reindex_shared.dyn_wmma.matrix_a_o"):
                                    v0_o = T.axis.spatial(1024, ax0_0_0_ax1_0_0_fused // 64 * 64 + ax0_0_1_ax1_0_1_fused // 2 * 16 + ax0_0_2_ax1_0_2_fused // 2 * 4 + ax0_0)
                                    v1_o = T.axis.spatial(1024, ax2_0_0 * 2 + ax1_0)
                                    T.reads(A_reindex_shared_dyn[v0_o * 16:v0_o * 16 + 16, v1_o * 16:v1_o * 16 + 16])
                                    T.writes(A_reindex_shared_dyn_wmma_matrix_a[v0_o * 16:v0_o * 16 + 16, v1_o * 16:v1_o * 16 + 16])
                                    A_1 = T.match_buffer(A_reindex_shared_dyn[v0_o * 16:v0_o * 16 + 16, v1_o * 16:v1_o * 16 + 16], (16, 16), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                    C_1 = T.match_buffer(A_reindex_shared_dyn_wmma_matrix_a[v0_o * 16:v0_o * 16 + 16, v1_o * 16:v1_o * 16 + 16], (16, 16), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_a", offset_factor=16)
                                    T.tvm_load_matrix_sync(C_1.data, 16, 16, 16, C_1.elem_offset // C_1.strides[0] // 16 * (C_1.strides[0] // 16) + C_1.elem_offset % C_1.strides[0] // 16, T.tvm_access_ptr(T.type_annotation("float16"), A_1.data, A_1.elem_offset, A_1.strides[0] * 16, 1), A_1.strides[0], "row_major")
                            for ax0_0, ax1_0 in T.grid(4, 2):
                                with T.block("B_decompress_reindex_shared.dyn_wmma.matrix_b_o"):
                                    v0_o = T.axis.spatial(1024, ax0_0_0_ax1_0_0_fused % 64 * 16 + ax0_0_1_ax1_0_1_fused % 2 * 8 + ax0_0_2_ax1_0_2_fused % 2 * 4 + ax0_0)
                                    v1_o = T.axis.spatial(1024, ax2_0_0 * 2 + ax1_0)
                                    T.reads(B_decompress_reindex_shared_dyn[v0_o * 16:v0_o * 16 + 16, v1_o * 16:v1_o * 16 + 16])
                                    T.writes(B_decompress_reindex_shared_dyn_wmma_matrix_b[v0_o * 16:v0_o * 16 + 16, v1_o * 16:v1_o * 16 + 16])
                                    A_1 = T.match_buffer(B_decompress_reindex_shared_dyn[v0_o * 16:v0_o * 16 + 16, v1_o * 16:v1_o * 16 + 16], (16, 16), "float16", strides=("A_s0", "A_s1"), scope="shared.dyn", offset_factor=16)
                                    C_1 = T.match_buffer(B_decompress_reindex_shared_dyn_wmma_matrix_b[v0_o * 16:v0_o * 16 + 16, v1_o * 16:v1_o * 16 + 16], (16, 16), "float16", strides=("C_s0", "C_s1"), scope="wmma.matrix_b", offset_factor=16)
                                    T.tvm_load_matrix_sync(C_1.data, 16, 16, 16, C_1.elem_offset // C_1.strides[0] // 16 * (C_1.strides[0] // 16) + C_1.elem_offset % C_1.strides[0] // 16, T.tvm_access_ptr(T.type_annotation("float16"), A_1.data, A_1.elem_offset, A_1.strides[0] * 16, 1), A_1.strides[0], "col_major")
                            for ax0_0_3, ax1_0_3, ax2_0_2, ax0_0_4, ax1_0_4 in T.grid(2, 2, 2, 2, 2):
                                with T.block("B_o_update"):
                                    v0_o = T.axis.spatial(1024, ax0_0_0_ax1_0_0_fused // 64 * 64 + ax0_0_1_ax1_0_1_fused // 2 * 16 + ax0_0_2_ax1_0_2_fused // 2 * 4 + ax0_0_3 * 2 + ax0_0_4)
                                    v1_o = T.axis.spatial(1024, ax0_0_0_ax1_0_0_fused % 64 * 16 + ax0_0_1_ax1_0_1_fused % 2 * 8 + ax0_0_2_ax1_0_2_fused % 2 * 4 + ax1_0_3 * 2 + ax1_0_4)
                                    v2_o = T.axis.reduce(1024, ax2_0_0 * 2 + ax2_0_1 * 2 + ax2_0_2)
                                    T.reads(C_reindex_shared_dyn_wmma_accumulator[v0_o // 4, v1_o // 4, v0_o % 4, v1_o % 4, 0:16, 0:16], A_reindex_shared_dyn_wmma_matrix_a[v0_o * 16:v0_o * 16 + 16, v2_o * 16:v2_o * 16 + 16], B_decompress_reindex_shared_dyn_wmma_matrix_b[v1_o * 16:v1_o * 16 + 16, v2_o * 16:v2_o * 16 + 16])
                                    T.writes(C_reindex_shared_dyn_wmma_accumulator[v0_o // 4, v1_o // 4, v0_o % 4, v1_o % 4, 0:16, 0:16])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(1024), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "warp_execution": T.int64(1)})
                                    A_1 = T.match_buffer(A_reindex_shared_dyn_wmma_matrix_a[v0_o * 16:v0_o * 16 + 16, v2_o * 16:v2_o * 16 + 16], (16, 16), "float16", strides=("A_s0", "A_s1"), scope="wmma.matrix_a", offset_factor=16)
                                    B_1 = T.match_buffer(B_decompress_reindex_shared_dyn_wmma_matrix_b[v1_o * 16:v1_o * 16 + 16, v2_o * 16:v2_o * 16 + 16], (16, 16), "float16", strides=("B_s0", "B_s1"), scope="wmma.matrix_b", offset_factor=16)
                                    C_1 = T.match_buffer(C_reindex_shared_dyn_wmma_accumulator[v0_o // 4, v1_o // 4, v0_o % 4, v1_o % 4, 0:16, 0:16], (16, 16), "float16", strides=("C_s0", "C_s1"), scope="wmma.accumulator", offset_factor=16)
                                    T.tvm_mma_sync(C_1.data, C_1.elem_offset // C_1.strides[0] // 16 * (C_1.strides[0] // 16) + C_1.elem_offset % C_1.strides[0] // 16, A_1.data, A_1.elem_offset // A_1.strides[0] // 16 * (A_1.strides[0] // 16) + A_1.elem_offset % A_1.strides[0] // 16, B_1.data, B_1.elem_offset // B_1.strides[0] // 16 * (B_1.strides[0] // 16) + B_1.elem_offset % B_1.strides[0] // 16, C_1.data, C_1.elem_offset // C_1.strides[0] // 16 * (C_1.strides[0] // 16) + C_1.elem_offset % C_1.strides[0] // 16)
                for ax2 in range(4):
                    for ax0_ax1_fused in T.thread_binding(8, thread="threadIdx.y"):
                        for ax2_1, ax3 in T.grid(1, 4):
                            with T.block("C_reindex_shared.dyn_wmma.accumulator_o"):
                                v0 = T.axis.spatial(256, ax0_0_0_ax1_0_0_fused // 64 * 16 + ax0_0_1_ax1_0_1_fused // 2 * 4 + ax0_ax1_fused // 2)
                                v1 = T.axis.spatial(256, ax0_0_0_ax1_0_0_fused % 64 * 4 + ax0_0_1_ax1_0_1_fused % 2 * 2 + ax0_ax1_fused % 2)
                                v2 = T.axis.spatial(4, ax2 + ax2_1)
                                v3 = T.axis.spatial(4, ax3)
                                v4_o = T.axis.spatial(1, 0)
                                v5_o = T.axis.spatial(1, 0)
                                T.reads(C_reindex_shared_dyn_wmma_accumulator[v0, v1, v2, v3, 0:16, 0:16])
                                T.writes(C_reindex_shared_dyn[v0, v1, v2, v3, 0:16, 0:16])
                                A_1 = T.match_buffer(C_reindex_shared_dyn_wmma_accumulator[v0, v1, v2, v3, 0:16, 0:16], (16, 16), "float16", strides=("A_s0", "A_s1"), scope="wmma.accumulator", offset_factor=16)
                                C_1 = T.match_buffer(C_reindex_shared_dyn[v0, v1, v2, v3, 0:16, 0:16], (16, 16), "float16", strides=("C_s0", "C_s1"), scope="shared.dyn", offset_factor=16)
                                T.tvm_store_matrix_sync(A_1.data, 16, 16, 16, A_1.elem_offset // A_1.strides[0] // 16 * (A_1.strides[0] // 16) + A_1.elem_offset % A_1.strides[0] // 16, T.tvm_access_ptr(T.type_annotation("float16"), C_1.data, C_1.elem_offset, C_1.strides[0] * 16, 2), C_1.strides[0], "row_major")
                    for ax0_ax1_ax3_ax4_ax5_fused_0 in range(8):
                        for ax0_ax1_ax3_ax4_ax5_fused_1 in T.thread_binding(8, thread="threadIdx.y"):
                            for ax0_ax1_ax3_ax4_ax5_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax3_ax4_ax5_fused_3 in T.vectorized(4):
                                    with T.block("C_reindex_shared.dyn"):
                                        v0 = T.axis.spatial(256, ax0_0_0_ax1_0_0_fused // 64 * 16 + ax0_0_1_ax1_0_1_fused // 2 * 4 + (ax0_ax1_ax3_ax4_ax5_fused_0 * 1024 + ax0_ax1_ax3_ax4_ax5_fused_1 * 128 + ax0_ax1_ax3_ax4_ax5_fused_2 * 4 + ax0_ax1_ax3_ax4_ax5_fused_3) // 2048)
                                        v1 = T.axis.spatial(256, ax0_0_0_ax1_0_0_fused % 64 * 4 + ax0_0_1_ax1_0_1_fused % 2 * 2 + (ax0_ax1_ax3_ax4_ax5_fused_0 * 1024 + ax0_ax1_ax3_ax4_ax5_fused_1 * 128 + ax0_ax1_ax3_ax4_ax5_fused_2 * 4 + ax0_ax1_ax3_ax4_ax5_fused_3) % 2048 // 1024)
                                        v2 = T.axis.spatial(4, ax2)
                                        v3 = T.axis.spatial(4, (ax0_ax1_ax3_ax4_ax5_fused_0 * 1024 + ax0_ax1_ax3_ax4_ax5_fused_1 * 128 + ax0_ax1_ax3_ax4_ax5_fused_2 * 4 + ax0_ax1_ax3_ax4_ax5_fused_3) % 1024 // 256)
                                        v4 = T.axis.spatial(16, (ax0_ax1_ax3_ax4_ax5_fused_0 * 1024 + ax0_ax1_ax3_ax4_ax5_fused_1 * 128 + ax0_ax1_ax3_ax4_ax5_fused_2 * 4 + ax0_ax1_ax3_ax4_ax5_fused_3) % 256 // 16)
                                        v5 = T.axis.spatial(16, (ax0_ax1_ax3_ax4_ax5_fused_0 * 1024 + ax0_ax1_ax3_ax4_ax5_fused_1 * 128 + ax0_ax1_ax3_ax4_ax5_fused_2 * 4 + ax0_ax1_ax3_ax4_ax5_fused_3) % 16)
                                        T.reads(C_reindex_shared_dyn[v0, v1, v2, v3, v4, v5])
                                        T.writes(C[v4 + v2 * 16 + v0 * 64, v5 + v3 * 16 + v1 * 64])
                                        C[v4 + v2 * 16 + v0 * 64, v5 + v3 * 16 + v1 * 64] = C_reindex_shared_dyn[v0, v1, v2, v3, v4, v5]
# from tvm import tir
def apply_trace(sch: tir.Schedule) -> None:
  b0 = sch.get_block(name="B_decompress", func_name="main")
  b1 = sch.get_block(name="B", func_name="main")
  b2 = sch.get_block(name="root", func_name="main")
  sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
  b3 = sch.reindex(block=b1, buffer=("write", 0))
  b4 = sch.reindex(block=b1, buffer=("read", 0))
  b5 = sch.reindex(block=b1, buffer=("read", 1))
  sch.transform_layout(block=b1, buffer=("read", 0), index_map=lambda vi, vk: (vi, vk,), pad_value=None, assume_injective_transform=True)
  sch.transform_layout(block=b1, buffer=("read", 1), index_map=lambda vj, vk: (vj, vk,), pad_value=None, assume_injective_transform=True)
  sch.transform_layout(block=b1, buffer=("write", 0), index_map=lambda vi, vj: (vi, vj,), pad_value=None, assume_injective_transform=True)
  sch.transform_block_layout(block=b3, index_map=lambda vi, vj: (vi, vj,))
  sch.transform_block_layout(block=b4, index_map=lambda vi, vk: (vi, vk,))
  sch.transform_block_layout(block=b5, index_map=lambda vj, vk: (vj, vk,))
  sch.transform_block_layout(block=b1, index_map=lambda vi, vj, vk: (vi, vj, vk,))
  l6, l7, l8 = sch.get_loops(block=b1)
  l9, l10 = sch.split(loop=l8, factors=[None, 16], preserve_unit_iters=True)
  l11, l12 = sch.split(loop=l7, factors=[None, 16], preserve_unit_iters=True)
  l13, l14 = sch.split(loop=l6, factors=[None, 16], preserve_unit_iters=True)
  l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b1)
  sch.reorder(l17, l19, l14, l12, l10)
  b21 = sch.blockize(loop=l14, preserve_unit_iters=True)
  sch.annotate(block_or_loop=b21, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_sync_16x16x16_f16f16f16_trans")
  sch.annotate(block_or_loop=b21, ann_key="meta_schedule.auto_tensorize_init", ann_val="wmma_fill_16x16x16_f16")
  sch.annotate(block_or_loop=b21, ann_key="warp_execution", ann_val=1)
  l22, l23, l24 = sch.get_loops(block=b21)
  v25, v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l22, n=5, max_innermost_factor=4, decision=[16, 4, 4, 2, 2])
  l30, l31, l32, l33, l34 = sch.split(loop=l22, factors=[v25, v26, v27, v28, v29], preserve_unit_iters=True)
  v35, v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l23, n=5, max_innermost_factor=4, decision=[64, 2, 2, 2, 2])
  l40, l41, l42, l43, l44 = sch.split(loop=l23, factors=[v35, v36, v37, v38, v39], preserve_unit_iters=True)
  v45, v46, v47 = sch.sample_perfect_tile(loop=l24, n=3, max_innermost_factor=4, decision=[512, 1, 2])
  l48, l49, l50 = sch.split(loop=l24, factors=[v45, v46, v47], preserve_unit_iters=True)
  sch.reorder(l30, l40, l31, l41, l32, l42, l48, l49, l33, l43, l50, l34, l44)
  l51 = sch.fuse(l30, l40, preserve_unit_iters=True)
  sch.bind(loop=l51, thread_axis="blockIdx.y")
  l52 = sch.fuse(l31, l41, preserve_unit_iters=True)
  sch.bind(loop=l52, thread_axis="blockIdx.x")
  l53 = sch.fuse(l32, l42, preserve_unit_iters=True)
  sch.bind(loop=l53, thread_axis="threadIdx.y")
  sch.annotate(block_or_loop=b21, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
  sch.annotate(block_or_loop=b21, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
  sch.transform_layout(block=b21, buffer=("write", 0), index_map=lambda i0, i1: (i0 // 16 // (v28 * v29), i1 // 16 // (v38 * v39), i0 // 16 % (v28 * v29), i1 // 16 % (v38 * v39), i0 % 16, i1 % 16,), pad_value=None, assume_injective_transform=True)
  b54 = sch.cache_write(block=b21, write_buffer_index=0, storage_scope="shared.dyn")
  sch.reverse_compute_at(block=b54, loop=l52, preserve_unit_loops=True, index=-1)
  b55 = sch.cache_write(block=b21, write_buffer_index=0, storage_scope="wmma.accumulator")
  l56, l57, l58, l59, l60, l61, l62, l63 = sch.get_loops(block=b54)
  sch.reorder(l60, l58, l59, l61)
  sch.compute_at(block=b55, loop=l60, preserve_unit_loops=True, index=-1)
  l64, l65, l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b55)
  l73 = sch.fuse(l67, l68, preserve_unit_iters=True)
  sch.bind(loop=l73, thread_axis="threadIdx.y")
  sch.reverse_compute_inline(block=b3)
  l74, l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b55)
  b82 = sch.blockize(loop=l80, preserve_unit_iters=True)
  sch.annotate(block_or_loop=b82, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_store_16x16x16_f16_shared_dyn")
  l83, l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b54)
  l91 = sch.fuse(l86, l87, l88, l89, l90, preserve_unit_iters=True)
  v92 = sch.sample_categorical(candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
  sch.annotate(block_or_loop=b54, ann_key="meta_schedule.cooperative_fetch", ann_val=v92)
  b93 = sch.cache_read(block=b21, read_buffer_index=0, storage_scope="shared.dyn", consumer_blocks=[b21])
  sch.compute_at(block=b93, loop=l48, preserve_unit_loops=True, index=-1)
  l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b93)
  l100 = sch.fuse(l98, l99, preserve_unit_iters=True)
  v101 = sch.sample_categorical(candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
  sch.annotate(block_or_loop=b93, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
  b102 = sch.cache_read(block=b21, read_buffer_index=1, storage_scope="shared.dyn", consumer_blocks=[b21])
  sch.compute_at(block=b102, loop=l48, preserve_unit_loops=True, index=-1)
  l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b102)
  l109 = sch.fuse(l107, l108, preserve_unit_iters=True)
  v110 = sch.sample_categorical(candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
  sch.annotate(block_or_loop=b102, ann_key="meta_schedule.cooperative_fetch", ann_val=v110)
  b111 = sch.cache_read(block=b21, read_buffer_index=0, storage_scope="wmma.matrix_a")
  sch.compute_at(block=b111, loop=l49, preserve_unit_loops=True, index=-1)
  l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b111)
  l119, l120 = sch.split(loop=l118, factors=[None, 16], preserve_unit_iters=True)
  l121, l122 = sch.split(loop=l117, factors=[None, 16], preserve_unit_iters=True)
  l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b111)
  sch.reorder(l130, l122, l120)
  b132 = sch.blockize(loop=l122, preserve_unit_iters=True)
  sch.annotate(block_or_loop=b132, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_load_16x16x16_f16_a_shared_dyn")
  b133 = sch.cache_read(block=b21, read_buffer_index=1, storage_scope="wmma.matrix_b")
  sch.compute_at(block=b133, loop=l49, preserve_unit_loops=True, index=-1)
  l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b133)
  l141, l142 = sch.split(loop=l140, factors=[None, 16], preserve_unit_iters=True)
  l143, l144 = sch.split(loop=l139, factors=[None, 16], preserve_unit_iters=True)
  l145, l146, l147, l148, l149, l150, l151, l152, l153 = sch.get_loops(block=b133)
  sch.reorder(l152, l144, l142)
  b154 = sch.blockize(loop=l144, preserve_unit_iters=True)
  sch.annotate(block_or_loop=b154, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_load_16x16x16_f16_b_trans_shared_dyn")
  sch.compute_inline(block=b4)
  sch.compute_inline(block=b5)
  sch.storage_align(block=b93, buffer_index=0, axis=-2, factor=32, offset=8)
  sch.storage_align(block=b102, buffer_index=0, axis=-2, factor=32, offset=8)
  sch.compute_inline(block=b0)
  v155 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
  sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v155)
  sch.enter_postproc()
  sch.unannotate(block_or_loop=b54, ann_key="meta_schedule.cooperative_fetch")
  l156, l157, l158, l159 = sch.get_loops(block=b54)
  l160, l161, l162, l163 = sch.split(loop=l159, factors=[None, 8, 32, 4], preserve_unit_iters=True)
  sch.vectorize(loop=l163)
  sch.bind(loop=l162, thread_axis="threadIdx.x")
  sch.bind(loop=l161, thread_axis="threadIdx.y")
  sch.unannotate(block_or_loop=b93, ann_key="meta_schedule.cooperative_fetch")
  l164, l165, l166, l167, l168 = sch.get_loops(block=b93)
  l169, l170, l171, l172 = sch.split(loop=l168, factors=[None, 8, 32, 4], preserve_unit_iters=True)
  sch.vectorize(loop=l172)
  sch.bind(loop=l171, thread_axis="threadIdx.x")
  sch.bind(loop=l170, thread_axis="threadIdx.y")
  sch.unannotate(block_or_loop=b102, ann_key="meta_schedule.cooperative_fetch")
  l173, l174, l175, l176, l177 = sch.get_loops(block=b102)
  l178, l179, l180, l181 = sch.split(loop=l177, factors=[None, 8, 32, 2], preserve_unit_iters=True)
  sch.vectorize(loop=l181)
  sch.bind(loop=l180, thread_axis="threadIdx.x")
  sch.bind(loop=l179, thread_axis="threadIdx.y")
  b182 = sch.get_block(name="root", func_name="main")
  sch.unannotate(block_or_loop=b182, ann_key="meta_schedule.unroll_explicit")
  b183, b184, b185, b186, b187, b188, b189 = sch.get_child_blocks(b182)
  l190, l191, l192, l193, l194, l195, l196, l197 = sch.get_loops(block=b183)
  sch.annotate(block_or_loop=l190, ann_key="pragma_auto_unroll_max_step", ann_val=16)
  sch.annotate(block_or_loop=l190, ann_key="pragma_unroll_explicit", ann_val=1)
  l198, l199, l200, l201, l202, l203, l204, l205 = sch.get_loops(block=b184)
  sch.annotate(block_or_loop=l198, ann_key="pragma_auto_unroll_max_step", ann_val=16)
  sch.annotate(block_or_loop=l198, ann_key="pragma_unroll_explicit", ann_val=1)
  l206, l207, l208, l209, l210, l211, l212 = sch.get_loops(block=b185)
  sch.annotate(block_or_loop=l206, ann_key="pragma_auto_unroll_max_step", ann_val=16)
  sch.annotate(block_or_loop=l206, ann_key="pragma_unroll_explicit", ann_val=1)
  l213, l214, l215, l216, l217, l218, l219 = sch.get_loops(block=b186)
  sch.annotate(block_or_loop=l213, ann_key="pragma_auto_unroll_max_step", ann_val=16)
  sch.annotate(block_or_loop=l213, ann_key="pragma_unroll_explicit", ann_val=1)
  l220, l221, l222, l223, l224, l225, l226, l227, l228, l229 = sch.get_loops(block=b187)
  sch.annotate(block_or_loop=l220, ann_key="pragma_auto_unroll_max_step", ann_val=16)
  sch.annotate(block_or_loop=l220, ann_key="pragma_unroll_explicit", ann_val=1)
  l230, l231, l232, l233, l234, l235 = sch.get_loops(block=b188)
  sch.annotate(block_or_loop=l230, ann_key="pragma_auto_unroll_max_step", ann_val=16)
  sch.annotate(block_or_loop=l230, ann_key="pragma_unroll_explicit", ann_val=1)
  l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b189)
  sch.annotate(block_or_loop=l236, ann_key="pragma_auto_unroll_max_step", ann_val=16)
  sch.annotate(block_or_loop=l236, ann_key="pragma_unroll_explicit", ann_val=1)
  b243 = sch.get_block(name="B_o", func_name="main")
  l244, l245, l246, l247, l248, l249, l250, l251, l252, l253 = sch.get_loops(block=b243)
  b254 = sch.decompose_reduction(block=b243, loop=l247)
  sch.unannotate(block_or_loop=b254, ann_key="meta_schedule.auto_tensorize")
  sch.annotate(block_or_loop=b254, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_fill_16x16x16_f16")
  sch.unannotate(block_or_loop=b243, ann_key="meta_schedule.auto_tensorize_init")
  sch.unannotate(block_or_loop=b254, ann_key="meta_schedule.auto_tensorize_init")
  b255 = sch.get_block(name="B_o_init", func_name="main")
  sch.unannotate(block_or_loop=b255, ann_key="meta_schedule.auto_tensorize")
  sch.tensorize(block_or_loop=b255, tensor_intrin="wmma_fill_16x16x16_f16", preserve_unit_iters=True)
  b256 = sch.get_block(name="A_reindex_shared.dyn_wmma.matrix_a_o", func_name="main")
  sch.unannotate(block_or_loop=b256, ann_key="meta_schedule.auto_tensorize")
  sch.tensorize(block_or_loop=b256, tensor_intrin="wmma_load_16x16x16_f16_a_shared_dyn", preserve_unit_iters=True)
  b257 = sch.get_block(name="B_decompress_reindex_shared.dyn_wmma.matrix_b_o", func_name="main")
  sch.unannotate(block_or_loop=b257, ann_key="meta_schedule.auto_tensorize")
  sch.tensorize(block_or_loop=b257, tensor_intrin="wmma_load_16x16x16_f16_b_trans_shared_dyn", preserve_unit_iters=True)
  b258 = sch.get_block(name="B_o_update", func_name="main")
  sch.unannotate(block_or_loop=b258, ann_key="meta_schedule.auto_tensorize")
  sch.tensorize(block_or_loop=b258, tensor_intrin="wmma_sync_16x16x16_f16f16f16_trans", preserve_unit_iters=True)
  b259 = sch.get_block(name="C_reindex_shared.dyn_wmma.accumulator_o", func_name="main")
  sch.unannotate(block_or_loop=b259, ann_key="meta_schedule.auto_tensorize")
  sch.tensorize(block_or_loop=b259, tensor_intrin="wmma_store_16x16x16_f16_shared_dyn", preserve_unit_iters=True)
