2023-05-02 13:37:31 [INFO] [task_scheduler.cc:160] Initializing Task #0: "main"
2023-05-02 13:37:31 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer((16, 16384), "float16"), B: T.Buffer((16384, 8192), "int8"), C: T.Buffer((16, 16384), "float16")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        B_decompress = T.alloc_buffer((16384, 16384), "float16")
        for i, j in T.grid(16384, 16384):
            with T.block("B_decompress"):
                vi, vj = T.axis.remap("SS", [i, j])
                T.reads(B[vi, vj // 2:vj // 2 + 2])
                T.writes(B_decompress[vi, vj])
                B_decompress[vi, vj] = T.Select(vj % 32 * 4 % 8 <= 5, T.Cast("float16", T.bitwise_and(T.shift_right(T.Cast("int32", B[vi, vj // 32 * 16 + vj % 32 * 4 // 8]), vj % 32 * 4 % 8), 15)), T.Cast("float16", T.bitwise_or(T.Cast("int8", T.bitwise_and(T.shift_right(T.Cast("int32", B[vi, vj // 32 * 16 + vj % 32 * 4 // 8]), vj % 32 * 4 % 8), T.shift_left(1, 8 - vj % 32 * 4 % 8) - 1)), T.Cast("int8", T.bitwise_and(T.bitwise_and(T.shift_left(T.Cast("int32", B[vi, vj // 32 * 16 + vj % 32 * 4 // 8 + 1]), 8 - vj % 32 * 4 % 8), T.shift_left(15, 8 - vj % 32 * 4 % 8)), 15)))))
        for i, j, k in T.grid(16, 16384, 16384):
            with T.block("B"):
                vi, vj, vk = T.axis.remap("SSR", [i, j, k])
                T.reads(A[vi, vk], B_decompress[vj, vk])
                T.writes(C[vi, vj])
                with T.init():
                    C[vi, vj] = T.float16(0)
                C[vi, vj] = C[vi, vj] + A[vi, vk] * B_decompress[vj, vk]
2023-05-02 13:37:31 [INFO] [multi_level_tiling_tensor_core.cc:216] Sketch 0: tensorizing with wmma_sync_16x16x16_f16f16f16_trans
2023-05-02 13:37:31 [INFO] [multi_level_tiling_tensor_core.cc:216] Sketch 1: tensorizing with wmma_sync_16x16x16_f16f16f16
2023-05-02 13:37:31 [INFO] [task_scheduler.cc:164] Total 2 design space(s) generated
2023-05-02 13:37:31 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer((16, 16384), "float16"), B: T.Buffer((16384, 8192), "int8"), C: T.Buffer((16, 16384), "float16")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 64})
            C_reindex_shared_dyn = T.alloc_buffer((1, 64, 1, 16, 16, 16), "float16", scope="shared.dyn")
            C_reindex_shared_dyn_wmma_accumulator = T.alloc_buffer((1, 64, 1, 16, 16, 16), "float16", scope="wmma.accumulator")
            A_reindex_shared_dyn = T.alloc_buffer((16, 16384), "float16", scope="shared.dyn")
            B_decompress_reindex_shared_dyn = T.alloc_buffer((16384, 16384), "float16", scope="shared.dyn")
            A_reindex_shared_dyn_wmma_matrix_a = T.alloc_buffer((16, 16384), "float16", scope="wmma.matrix_a")
            B_decompress_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((16384, 16384), "float16", scope="wmma.matrix_b")
            for ax0_0_0_ax1_0_0_fused in T.thread_binding(2, thread="blockIdx.y"):
                for ax0_0_1_ax1_0_1_fused in T.thread_binding(32, thread="blockIdx.x"):
                    for ax0_0_2_ax1_0_2_fused in T.thread_binding(1, thread="threadIdx.y"):
                        for ax2_0_0 in range(64):
                            for ax0_ax1_fused in range(4096):
                                with T.block("A_reindex_shared.dyn"):
                                    v0 = T.axis.spatial(16, ax0_ax1_fused // 256)
                                    v1 = T.axis.spatial(16384, ax2_0_0 * 256 + ax0_ax1_fused % 256)
                                    T.reads(A[v0, v1])
                                    T.writes(A_reindex_shared_dyn[v0, v1])
                                    T.block_attr({"buffer_dim_align": [[0, 0, 32, 8]], "meta_schedule.cooperative_fetch": 4})
                                    A_reindex_shared_dyn[v0, v1] = A[v0, v1]
                            for ax0_ax1_fused in range(65536):
                                with T.block("B_decompress_reindex_shared.dyn"):
                                    v0 = T.axis.spatial(16384, ax2_0_0 * 256 + ax0_ax1_fused // 256)
                                    v1 = T.axis.spatial(16384, ax0_0_0_ax1_0_0_fused * 8192 + ax0_0_1_ax1_0_1_fused * 256 + ax0_ax1_fused % 256)
                                    T.reads(B[v1, v0 // 2:v0 // 2 + 2])
                                    T.writes(B_decompress_reindex_shared_dyn[v0, v1])
                                    T.block_attr({"buffer_dim_align": [[0, 0, 32, 8]], "meta_schedule.cooperative_fetch": 8})
                                    B_decompress_reindex_shared_dyn[v0, v1] = T.Select(v0 % 32 * 4 % 8 <= 5, T.Cast("float16", T.bitwise_and(T.shift_right(T.Cast("int32", B[v1, v0 // 32 * 16 + v0 % 32 * 4 // 8]), v0 % 32 * 4 % 8), 15)), T.Cast("float16", T.bitwise_or(T.Cast("int8", T.bitwise_and(T.shift_right(T.Cast("int32", B[v1, v0 // 32 * 16 + v0 % 32 * 4 // 8]), v0 % 32 * 4 % 8), T.shift_left(1, 8 - v0 % 32 * 4 % 8) - 1)), T.Cast("int8", T.bitwise_and(T.bitwise_and(T.shift_left(T.Cast("int32", B[v1, v0 // 32 * 16 + v0 % 32 * 4 // 8 + 1]), 8 - v0 % 32 * 4 % 8), T.shift_left(15, 8 - v0 % 32 * 4 % 8)), 15)))))
                            for ax2_0_1 in range(8):
                                for ax0_0, ax1_0 in T.grid(1, 2):
                                    with T.block("A_reindex_shared.dyn_wmma.matrix_a_o"):
                                        v0_o = T.axis.spatial(1, ax0_0)
                                        v1_o = T.axis.spatial(1024, ax2_0_0 * 16 + ax2_0_1 * 2 + ax1_0)
                                        T.reads(A_reindex_shared_dyn[0:16, v1_o * 16:v1_o * 16 + 16])
                                        T.writes(A_reindex_shared_dyn_wmma_matrix_a[0:16, v1_o * 16:v1_o * 16 + 16])
                                        T.block_attr({"meta_schedule.auto_tensorize": "wmma_load_16x16x16_f16_a_shared_dyn"})
                                        for ax0_1, ax1_1 in T.grid(16, 16):
                                            with T.block("A_reindex_shared.dyn_wmma.matrix_a"):
                                                v0_i, v1_i = T.axis.remap("SS", [ax0_1, ax1_1])
                                                T.reads(A_reindex_shared_dyn[v0_i, v1_o * 16 + v1_i])
                                                T.writes(A_reindex_shared_dyn_wmma_matrix_a[v0_i, v1_o * 16 + v1_i])
                                                A_reindex_shared_dyn_wmma_matrix_a[v0_i, v1_o * 16 + v1_i] = A_reindex_shared_dyn[v0_i, v1_o * 16 + v1_i]
                                for ax0_0, ax1_0 in T.grid(2, 16):
                                    with T.block("B_decompress_reindex_shared.dyn_wmma.matrix_b_o"):
                                        v0_o = T.axis.spatial(1024, ax2_0_0 * 16 + ax2_0_1 * 2 + ax0_0)
                                        v1_o = T.axis.spatial(1024, ax0_0_0_ax1_0_0_fused * 512 + ax0_0_1_ax1_0_1_fused * 16 + ax1_0)
                                        T.reads(B_decompress_reindex_shared_dyn[v0_o * 16:v0_o * 16 + 16, v1_o * 16:v1_o * 16 + 16])
                                        T.writes(B_decompress_reindex_shared_dyn_wmma_matrix_b[v0_o * 16:v0_o * 16 + 16, v1_o * 16:v1_o * 16 + 16])
                                        T.block_attr({"meta_schedule.auto_tensorize": "wmma_load_16x16x16_f16_b_shared_dyn"})
                                        for ax0_1, ax1_1 in T.grid(16, 16):
                                            with T.block("B_decompress_reindex_shared.dyn_wmma.matrix_b"):
                                                v0_i, v1_i = T.axis.remap("SS", [ax0_1, ax1_1])
                                                T.reads(B_decompress_reindex_shared_dyn[v0_o * 16 + v0_i, v1_o * 16 + v1_i])
                                                T.writes(B_decompress_reindex_shared_dyn_wmma_matrix_b[v0_o * 16 + v0_i, v1_o * 16 + v1_i])
                                                B_decompress_reindex_shared_dyn_wmma_matrix_b[v0_o * 16 + v0_i, v1_o * 16 + v1_i] = B_decompress_reindex_shared_dyn[v0_o * 16 + v0_i, v1_o * 16 + v1_i]
                                for ax0_0_3, ax1_0_3, ax2_0_2, ax0_0_4, ax1_0_4 in T.grid(1, 8, 2, 1, 2):
                                    with T.block("B_o"):
                                        v0_o = T.axis.spatial(1, ax0_0_4 + ax0_0_3)
                                        v1_o = T.axis.spatial(1024, ax0_0_0_ax1_0_0_fused * 512 + ax0_0_1_ax1_0_1_fused * 16 + ax1_0_3 * 2 + ax1_0_4)
                                        v2_o = T.axis.reduce(1024, ax2_0_0 * 16 + ax2_0_1 * 2 + ax2_0_2)
                                        T.reads(A_reindex_shared_dyn_wmma_matrix_a[0:16, v2_o * 16:v2_o * 16 + 16], B_decompress_reindex_shared_dyn_wmma_matrix_b[v2_o * 16:v2_o * 16 + 16, v1_o * 16:v1_o * 16 + 16])
                                        T.writes(C_reindex_shared_dyn_wmma_accumulator[0, v1_o // 16, 0, v1_o % 16, 0:16, 0:16])
                                        T.block_attr({"meta_schedule.auto_tensorize": "wmma_sync_16x16x16_f16f16f16", "meta_schedule.auto_tensorize_init": "wmma_fill_16x16x16_f16", "meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "warp_execution": 1})
                                        with T.init():
                                            for ax0_1, ax1_1 in T.grid(16, 16):
                                                with T.block("B_init"):
                                                    v0_i_init, v1_i_init = T.axis.remap("SS", [ax0_1, ax1_1])
                                                    T.reads()
                                                    T.writes(C_reindex_shared_dyn_wmma_accumulator[v0_i_init // 16, v1_o // 16, 0, v1_o % 16, v0_i_init, v1_i_init])
                                                    C_reindex_shared_dyn_wmma_accumulator[v0_i_init // 16, v1_o // 16, 0, v1_o % 16, v0_i_init, v1_i_init] = T.float16(0)
                                        for ax0_1, ax1_1, ax2_1 in T.grid(16, 16, 16):
                                            with T.block("B"):
                                                v0_i, v1_i, v2_i = T.axis.remap("SSR", [ax0_1, ax1_1, ax2_1])
                                                T.reads(C_reindex_shared_dyn_wmma_accumulator[v0_i // 16, v1_o // 16, 0, v1_o % 16, v0_i, v1_i], A_reindex_shared_dyn_wmma_matrix_a[v0_i, v2_o * 16 + v2_i], B_decompress_reindex_shared_dyn_wmma_matrix_b[v2_o * 16 + v2_i, v1_o * 16 + v1_i])
                                                T.writes(C_reindex_shared_dyn_wmma_accumulator[v0_i // 16, v1_o // 16, 0, v1_o % 16, v0_i, v1_i])
                                                T.block_attr({"meta_schedule.tiling_structure": "SSSRRSRS"})
                                                C_reindex_shared_dyn_wmma_accumulator[v0_i // 16, v1_o // 16, 0, v1_o % 16, v0_i, v1_i] = C_reindex_shared_dyn_wmma_accumulator[v0_i // 16, v1_o // 16, 0, v1_o % 16, v0_i, v1_i] + A_reindex_shared_dyn_wmma_matrix_a[v0_i, v2_o * 16 + v2_i] * B_decompress_reindex_shared_dyn_wmma_matrix_b[v2_o * 16 + v2_i, v1_o * 16 + v1_i]
                    for ax2 in range(1):
                        for ax0_ax1_fused in T.thread_binding(1, thread="threadIdx.y"):
                            for ax2_1, ax3 in T.grid(1, 16):
                                with T.block("C_reindex_shared.dyn_wmma.accumulator_o"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_0_0_ax1_0_0_fused * 32 + ax0_0_1_ax1_0_1_fused)
                                    v2, v3 = T.axis.remap("SS", [ax2_1, ax3])
                                    v4_o = T.axis.spatial(1, 0)
                                    v5_o = T.axis.spatial(1, 0)
                                    T.reads(C_reindex_shared_dyn_wmma_accumulator[v0, v1, v2, v3, 0:16, 0:16])
                                    T.writes(C_reindex_shared_dyn[v0, v1, v2, v3, 0:16, 0:16])
                                    T.block_attr({"meta_schedule.auto_tensorize": "wmma_store_16x16x16_f16_shared_dyn"})
                                    for ax4, ax5 in T.grid(16, 16):
                                        with T.block("C_reindex_shared.dyn_wmma.accumulator"):
                                            v4_i, v5_i = T.axis.remap("SS", [ax4, ax5])
                                            T.reads(C_reindex_shared_dyn_wmma_accumulator[v0, v1, v2, v3, v4_i, v5_i])
                                            T.writes(C_reindex_shared_dyn[v0, v1, v2, v3, v4_i, v5_i])
                                            C_reindex_shared_dyn[v0, v1, v2, v3, v4_i, v5_i] = C_reindex_shared_dyn_wmma_accumulator[v0, v1, v2, v3, v4_i, v5_i]
                        for ax0_ax1_ax3_ax4_ax5_fused in range(4096):
                            with T.block("C_reindex_shared.dyn"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(64, ax0_0_0_ax1_0_0_fused * 32 + ax0_0_1_ax1_0_1_fused)
                                v2 = T.axis.spatial(1, ax2)
                                v3 = T.axis.spatial(16, ax0_ax1_ax3_ax4_ax5_fused // 256)
                                v4 = T.axis.spatial(16, ax0_ax1_ax3_ax4_ax5_fused % 256 // 16)
                                v5 = T.axis.spatial(16, ax0_ax1_ax3_ax4_ax5_fused % 16)
                                T.reads(C_reindex_shared_dyn[v0, v1, v2, v3, v4, v5])
                                T.writes(C[v4 + v0 * 16, v5 + v3 * 16 + v1 * 256])
                                T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                C[v4 + v0 * 16, v5 + v3 * 16 + v1 * 256] = C_reindex_shared_dyn[v0, v1, v2, v3, v4, v5]
b0 = sch.get_block(name="B_decompress", func_name="main")
b1 = sch.get_block(name="B", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
b3 = sch.reindex(block=b1, buffer=("write", 0))
b4 = sch.reindex(block=b1, buffer=("read", 0))
b5 = sch.reindex(block=b1, buffer=("read", 1))
sch.transform_layout(block=b1, buffer=("read", 0), index_map=lambda vi, vk: (vi, vk,), pad_value=None, assume_injective_transform=True)
sch.transform_layout(block=b1, buffer=("read", 1), index_map=lambda vj, vk: (vk, vj,), pad_value=None, assume_injective_transform=True)
sch.transform_layout(block=b1, buffer=("write", 0), index_map=lambda vi, vj: (vi, vj,), pad_value=None, assume_injective_transform=True)
sch.transform_block_layout(block=b3, index_map=lambda vi, vj: (vi, vj,))
sch.transform_block_layout(block=b4, index_map=lambda vi, vk: (vi, vk,))
sch.transform_block_layout(block=b5, index_map=lambda vj, vk: (vk, vj,))
sch.transform_block_layout(block=b1, index_map=lambda vi, vj, vk: (vi, vj, vk,))
l6, l7, l8 = sch.get_loops(block=b1)
l9, l10 = sch.split(loop=l8, factors=[None, 16], preserve_unit_iters=True)
l11, l12 = sch.split(loop=l7, factors=[None, 16], preserve_unit_iters=True)
l13, l14 = sch.split(loop=l6, factors=[None, 16], preserve_unit_iters=True)
l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b1)
sch.reorder(l17, l19, l14, l12, l10)
b21 = sch.blockize(loop=l14, preserve_unit_iters=True)
sch.annotate(block_or_loop=b21, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_sync_16x16x16_f16f16f16")
sch.annotate(block_or_loop=b21, ann_key="meta_schedule.auto_tensorize_init", ann_val="wmma_fill_16x16x16_f16")
sch.annotate(block_or_loop=b21, ann_key="warp_execution", ann_val=1)
l22, l23, l24 = sch.get_loops(block=b21)
v25, v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l22, n=5, max_innermost_factor=4, decision=[1, 1, 1, 1, 1])
l30, l31, l32, l33, l34 = sch.split(loop=l22, factors=[v25, v26, v27, v28, v29], preserve_unit_iters=True)
v35, v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l23, n=5, max_innermost_factor=4, decision=[2, 32, 1, 8, 2])
l40, l41, l42, l43, l44 = sch.split(loop=l23, factors=[v35, v36, v37, v38, v39], preserve_unit_iters=True)
v45, v46, v47 = sch.sample_perfect_tile(loop=l24, n=3, max_innermost_factor=4, decision=[64, 8, 2])
l48, l49, l50 = sch.split(loop=l24, factors=[v45, v46, v47], preserve_unit_iters=True)
sch.reorder(l30, l40, l31, l41, l32, l42, l48, l49, l33, l43, l50, l34, l44)
l51 = sch.fuse(l30, l40, preserve_unit_iters=True)
sch.bind(loop=l51, thread_axis="blockIdx.y")
l52 = sch.fuse(l31, l41, preserve_unit_iters=True)
sch.bind(loop=l52, thread_axis="blockIdx.x")
l53 = sch.fuse(l32, l42, preserve_unit_iters=True)
sch.bind(loop=l53, thread_axis="threadIdx.y")
sch.annotate(block_or_loop=b21, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b21, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
sch.transform_layout(block=b21, buffer=("write", 0), index_map=lambda i0, i1: (i0 // 16 // (v28 * v29), i1 // 16 // (v38 * v39), i0 // 16 % (v28 * v29), i1 // 16 % (v38 * v39), i0 % 16, i1 % 16,), pad_value=None, assume_injective_transform=True)
b54 = sch.cache_write(block=b21, write_buffer_index=0, storage_scope="shared.dyn")
sch.reverse_compute_at(block=b54, loop=l52, preserve_unit_loops=True, index=-1)
b55 = sch.cache_write(block=b21, write_buffer_index=0, storage_scope="wmma.accumulator")
l56, l57, l58, l59, l60, l61, l62, l63 = sch.get_loops(block=b54)
sch.reorder(l60, l58, l59, l61)
sch.compute_at(block=b55, loop=l60, preserve_unit_loops=True, index=-1)
l64, l65, l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b55)
l73 = sch.fuse(l67, l68, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.y")
sch.reverse_compute_inline(block=b3)
l74, l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b55)
b82 = sch.blockize(loop=l80, preserve_unit_iters=True)
sch.annotate(block_or_loop=b82, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_store_16x16x16_f16_shared_dyn")
l83, l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b54)
l91 = sch.fuse(l86, l87, l88, l89, l90, preserve_unit_iters=True)
v92 = sch.sample_categorical(candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b54, ann_key="meta_schedule.cooperative_fetch", ann_val=v92)
b93 = sch.cache_read(block=b21, read_buffer_index=0, storage_scope="shared.dyn", consumer_blocks=[b21])
sch.compute_at(block=b93, loop=l48, preserve_unit_loops=True, index=-1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b93)
l100 = sch.fuse(l98, l99, preserve_unit_iters=True)
v101 = sch.sample_categorical(candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b93, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
b102 = sch.cache_read(block=b21, read_buffer_index=1, storage_scope="shared.dyn", consumer_blocks=[b21])
sch.compute_at(block=b102, loop=l48, preserve_unit_loops=True, index=-1)
l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b102)
l109 = sch.fuse(l107, l108, preserve_unit_iters=True)
v110 = sch.sample_categorical(candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b102, ann_key="meta_schedule.cooperative_fetch", ann_val=v110)
b111 = sch.cache_read(block=b21, read_buffer_index=0, storage_scope="wmma.matrix_a")
sch.compute_at(block=b111, loop=l49, preserve_unit_loops=True, index=-1)
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b111)
l119, l120 = sch.split(loop=l118, factors=[None, 16], preserve_unit_iters=True)
l121, l122 = sch.split(loop=l117, factors=[None, 16], preserve_unit_iters=True)
l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b111)
sch.reorder(l130, l122, l120)
b132 = sch.blockize(loop=l122, preserve_unit_iters=True)
sch.annotate(block_or_loop=b132, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_load_16x16x16_f16_a_shared_dyn")
b133 = sch.cache_read(block=b21, read_buffer_index=1, storage_scope="wmma.matrix_b")
sch.compute_at(block=b133, loop=l49, preserve_unit_loops=True, index=-1)
l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b133)
l141, l142 = sch.split(loop=l140, factors=[None, 16], preserve_unit_iters=True)
l143, l144 = sch.split(loop=l139, factors=[None, 16], preserve_unit_iters=True)
l145, l146, l147, l148, l149, l150, l151, l152, l153 = sch.get_loops(block=b133)
sch.reorder(l152, l144, l142)
b154 = sch.blockize(loop=l144, preserve_unit_iters=True)
sch.annotate(block_or_loop=b154, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_load_16x16x16_f16_b_shared_dyn")
sch.compute_inline(block=b4)
sch.compute_inline(block=b5)
sch.storage_align(block=b93, buffer_index=0, axis=-2, factor=32, offset=8)
sch.storage_align(block=b102, buffer_index=0, axis=-2, factor=32, offset=8)
sch.compute_inline(block=b0)
v155 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v155)
2023-05-02 13:37:31 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer((16, 16384), "float16"), B: T.Buffer((16384, 8192), "int8"), C: T.Buffer((16, 16384), "float16")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            C_reindex_shared_dyn = T.alloc_buffer((1, 256, 1, 4, 16, 16), "float16", scope="shared.dyn")
            C_reindex_shared_dyn_wmma_accumulator = T.alloc_buffer((1, 256, 1, 4, 16, 16), "float16", scope="wmma.accumulator")
            A_reindex_shared_dyn = T.alloc_buffer((16, 16384), "float16", scope="shared.dyn")
            B_decompress_reindex_shared_dyn = T.alloc_buffer((16384, 16384), "float16", scope="shared.dyn")
            A_reindex_shared_dyn_wmma_matrix_a = T.alloc_buffer((16, 16384), "float16", scope="wmma.matrix_a")
            B_decompress_reindex_shared_dyn_wmma_matrix_b = T.alloc_buffer((16384, 16384), "float16", scope="wmma.matrix_b")
            for ax0_0_0_ax1_0_0_fused in T.thread_binding(2, thread="blockIdx.y"):
                for ax0_0_1_ax1_0_1_fused in T.thread_binding(4, thread="blockIdx.x"):
                    for ax0_0_2_ax1_0_2_fused in T.thread_binding(32, thread="threadIdx.y"):
                        for ax2_0_0 in range(256):
                            for ax0_ax1_fused in range(1024):
                                with T.block("A_reindex_shared.dyn"):
                                    v0 = T.axis.spatial(16, ax0_ax1_fused // 64)
                                    v1 = T.axis.spatial(16384, ax2_0_0 * 64 + ax0_ax1_fused % 64)
                                    T.reads(A[v0, v1])
                                    T.writes(A_reindex_shared_dyn[v0, v1])
                                    T.block_attr({"buffer_dim_align": [[0, 0, 32, 8]], "meta_schedule.cooperative_fetch": 8})
                                    A_reindex_shared_dyn[v0, v1] = A[v0, v1]
                            for ax0_ax1_fused in range(131072):
                                with T.block("B_decompress_reindex_shared.dyn"):
                                    v0 = T.axis.spatial(16384, ax0_0_0_ax1_0_0_fused * 8192 + ax0_0_1_ax1_0_1_fused * 2048 + ax0_ax1_fused // 64)
                                    v1 = T.axis.spatial(16384, ax2_0_0 * 64 + ax0_ax1_fused % 64)
                                    T.reads(B[v0, v1 // 2:v1 // 2 + 2])
                                    T.writes(B_decompress_reindex_shared_dyn[v0, v1])
                                    T.block_attr({"buffer_dim_align": [[0, 0, 32, 8]], "meta_schedule.cooperative_fetch": 4})
                                    B_decompress_reindex_shared_dyn[v0, v1] = T.Select(v1 % 32 * 4 % 8 <= 5, T.Cast("float16", T.bitwise_and(T.shift_right(T.Cast("int32", B[v0, v1 // 32 * 16 + v1 % 32 * 4 // 8]), v1 % 32 * 4 % 8), 15)), T.Cast("float16", T.bitwise_or(T.Cast("int8", T.bitwise_and(T.shift_right(T.Cast("int32", B[v0, v1 // 32 * 16 + v1 % 32 * 4 // 8]), v1 % 32 * 4 % 8), T.shift_left(1, 8 - v1 % 32 * 4 % 8) - 1)), T.Cast("int8", T.bitwise_and(T.bitwise_and(T.shift_left(T.Cast("int32", B[v0, v1 // 32 * 16 + v1 % 32 * 4 // 8 + 1]), 8 - v1 % 32 * 4 % 8), T.shift_left(15, 8 - v1 % 32 * 4 % 8)), 15)))))
                            for ax2_0_1 in range(1):
                                for ax0_0, ax1_0 in T.grid(1, 4):
                                    with T.block("A_reindex_shared.dyn_wmma.matrix_a_o"):
                                        v0_o = T.axis.spatial(1, ax0_0)
                                        v1_o = T.axis.spatial(1024, ax2_0_0 * 4 + ax1_0)
                                        T.reads(A_reindex_shared_dyn[0:16, v1_o * 16:v1_o * 16 + 16])
                                        T.writes(A_reindex_shared_dyn_wmma_matrix_a[0:16, v1_o * 16:v1_o * 16 + 16])
                                        T.block_attr({"meta_schedule.auto_tensorize": "wmma_load_16x16x16_f16_a_shared_dyn"})
                                        for ax0_1, ax1_1 in T.grid(16, 16):
                                            with T.block("A_reindex_shared.dyn_wmma.matrix_a"):
                                                v0_i, v1_i = T.axis.remap("SS", [ax0_1, ax1_1])
                                                T.reads(A_reindex_shared_dyn[v0_i, v1_o * 16 + v1_i])
                                                T.writes(A_reindex_shared_dyn_wmma_matrix_a[v0_i, v1_o * 16 + v1_i])
                                                A_reindex_shared_dyn_wmma_matrix_a[v0_i, v1_o * 16 + v1_i] = A_reindex_shared_dyn[v0_i, v1_o * 16 + v1_i]
                                for ax0_0, ax1_0 in T.grid(4, 4):
                                    with T.block("B_decompress_reindex_shared.dyn_wmma.matrix_b_o"):
                                        v0_o = T.axis.spatial(1024, ax0_0_0_ax1_0_0_fused * 512 + ax0_0_1_ax1_0_1_fused * 128 + ax0_0_2_ax1_0_2_fused * 4 + ax0_0)
                                        v1_o = T.axis.spatial(1024, ax2_0_0 * 4 + ax1_0)
                                        T.reads(B_decompress_reindex_shared_dyn[v0_o * 16:v0_o * 16 + 16, v1_o * 16:v1_o * 16 + 16])
                                        T.writes(B_decompress_reindex_shared_dyn_wmma_matrix_b[v0_o * 16:v0_o * 16 + 16, v1_o * 16:v1_o * 16 + 16])
                                        T.block_attr({"meta_schedule.auto_tensorize": "wmma_load_16x16x16_f16_b_trans_shared_dyn"})
                                        for ax0_1, ax1_1 in T.grid(16, 16):
                                            with T.block("B_decompress_reindex_shared.dyn_wmma.matrix_b"):
                                                v0_i, v1_i = T.axis.remap("SS", [ax0_1, ax1_1])
                                                T.reads(B_decompress_reindex_shared_dyn[v0_o * 16 + v0_i, v1_o * 16 + v1_i])
                                                T.writes(B_decompress_reindex_shared_dyn_wmma_matrix_b[v0_o * 16 + v0_i, v1_o * 16 + v1_i])
                                                B_decompress_reindex_shared_dyn_wmma_matrix_b[v0_o * 16 + v0_i, v1_o * 16 + v1_i] = B_decompress_reindex_shared_dyn[v0_o * 16 + v0_i, v1_o * 16 + v1_i]
                                for ax0_0_3, ax1_0_3, ax2_0_2, ax0_0_4, ax1_0_4 in T.grid(1, 1, 4, 1, 4):
                                    with T.block("B_o"):
                                        v0_o = T.axis.spatial(1, ax0_0_4 + ax0_0_3)
                                        v1_o = T.axis.spatial(1024, ax0_0_0_ax1_0_0_fused * 512 + ax0_0_1_ax1_0_1_fused * 128 + ax0_0_2_ax1_0_2_fused * 4 + ax1_0_3 * 4 + ax1_0_4)
                                        v2_o = T.axis.reduce(1024, ax2_0_0 * 4 + ax2_0_1 * 4 + ax2_0_2)
                                        T.reads(A_reindex_shared_dyn_wmma_matrix_a[0:16, v2_o * 16:v2_o * 16 + 16], B_decompress_reindex_shared_dyn_wmma_matrix_b[v1_o * 16:v1_o * 16 + 16, v2_o * 16:v2_o * 16 + 16])
                                        T.writes(C_reindex_shared_dyn_wmma_accumulator[0, v1_o // 4, 0, v1_o % 4, 0:16, 0:16])
                                        T.block_attr({"meta_schedule.auto_tensorize": "wmma_sync_16x16x16_f16f16f16_trans", "meta_schedule.auto_tensorize_init": "wmma_fill_16x16x16_f16", "meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "warp_execution": 1})
                                        with T.init():
                                            for ax0_1, ax1_1 in T.grid(16, 16):
                                                with T.block("B_init"):
                                                    v0_i_init, v1_i_init = T.axis.remap("SS", [ax0_1, ax1_1])
                                                    T.reads()
                                                    T.writes(C_reindex_shared_dyn_wmma_accumulator[v0_i_init // 16, v1_o // 4, 0, v1_o % 4, v0_i_init, v1_i_init])
                                                    C_reindex_shared_dyn_wmma_accumulator[v0_i_init // 16, v1_o // 4, 0, v1_o % 4, v0_i_init, v1_i_init] = T.float16(0)
                                        for ax0_1, ax1_1, ax2_1 in T.grid(16, 16, 16):
                                            with T.block("B"):
                                                v0_i, v1_i, v2_i = T.axis.remap("SSR", [ax0_1, ax1_1, ax2_1])
                                                T.reads(C_reindex_shared_dyn_wmma_accumulator[v0_i // 16, v1_o // 4, 0, v1_o % 4, v0_i, v1_i], A_reindex_shared_dyn_wmma_matrix_a[v0_i, v2_o * 16 + v2_i], B_decompress_reindex_shared_dyn_wmma_matrix_b[v1_o * 16 + v1_i, v2_o * 16 + v2_i])
                                                T.writes(C_reindex_shared_dyn_wmma_accumulator[v0_i // 16, v1_o // 4, 0, v1_o % 4, v0_i, v1_i])
                                                T.block_attr({"meta_schedule.tiling_structure": "SSSRRSRS"})
                                                C_reindex_shared_dyn_wmma_accumulator[v0_i // 16, v1_o // 4, 0, v1_o % 4, v0_i, v1_i] = C_reindex_shared_dyn_wmma_accumulator[v0_i // 16, v1_o // 4, 0, v1_o % 4, v0_i, v1_i] + A_reindex_shared_dyn_wmma_matrix_a[v0_i, v2_o * 16 + v2_i] * B_decompress_reindex_shared_dyn_wmma_matrix_b[v1_o * 16 + v1_i, v2_o * 16 + v2_i]
                    for ax2 in range(1):
                        for ax0_ax1_fused in T.thread_binding(32, thread="threadIdx.y"):
                            for ax2_1, ax3 in T.grid(1, 4):
                                with T.block("C_reindex_shared.dyn_wmma.accumulator_o"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, ax0_0_0_ax1_0_0_fused * 128 + ax0_0_1_ax1_0_1_fused * 32 + ax0_ax1_fused)
                                    v2, v3 = T.axis.remap("SS", [ax2_1, ax3])
                                    v4_o = T.axis.spatial(1, 0)
                                    v5_o = T.axis.spatial(1, 0)
                                    T.reads(C_reindex_shared_dyn_wmma_accumulator[v0, v1, v2, v3, 0:16, 0:16])
                                    T.writes(C_reindex_shared_dyn[v0, v1, v2, v3, 0:16, 0:16])
                                    T.block_attr({"meta_schedule.auto_tensorize": "wmma_store_16x16x16_f16_shared_dyn"})
                                    for ax4, ax5 in T.grid(16, 16):
                                        with T.block("C_reindex_shared.dyn_wmma.accumulator"):
                                            v4_i, v5_i = T.axis.remap("SS", [ax4, ax5])
                                            T.reads(C_reindex_shared_dyn_wmma_accumulator[v0, v1, v2, v3, v4_i, v5_i])
                                            T.writes(C_reindex_shared_dyn[v0, v1, v2, v3, v4_i, v5_i])
                                            C_reindex_shared_dyn[v0, v1, v2, v3, v4_i, v5_i] = C_reindex_shared_dyn_wmma_accumulator[v0, v1, v2, v3, v4_i, v5_i]
                        for ax0_ax1_ax3_ax4_ax5_fused in range(32768):
                            with T.block("C_reindex_shared.dyn"):
                                v0 = T.axis.spatial(1, 0)
                                v1 = T.axis.spatial(256, ax0_0_0_ax1_0_0_fused * 128 + ax0_0_1_ax1_0_1_fused * 32 + ax0_ax1_ax3_ax4_ax5_fused // 1024)
                                v2 = T.axis.spatial(1, ax2)
                                v3 = T.axis.spatial(4, ax0_ax1_ax3_ax4_ax5_fused % 1024 // 256)
                                v4 = T.axis.spatial(16, ax0_ax1_ax3_ax4_ax5_fused % 256 // 16)
                                v5 = T.axis.spatial(16, ax0_ax1_ax3_ax4_ax5_fused % 16)
                                T.reads(C_reindex_shared_dyn[v0, v1, v2, v3, v4, v5])
                                T.writes(C[v4 + v0 * 16, v5 + v3 * 16 + v1 * 64])
                                T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                C[v4 + v0 * 16, v5 + v3 * 16 + v1 * 64] = C_reindex_shared_dyn[v0, v1, v2, v3, v4, v5]
b0 = sch.get_block(name="B_decompress", func_name="main")
b1 = sch.get_block(name="B", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
b3 = sch.reindex(block=b1, buffer=("write", 0))
b4 = sch.reindex(block=b1, buffer=("read", 0))
b5 = sch.reindex(block=b1, buffer=("read", 1))
sch.transform_layout(block=b1, buffer=("read", 0), index_map=lambda vi, vk: (vi, vk,), pad_value=None, assume_injective_transform=True)
sch.transform_layout(block=b1, buffer=("read", 1), index_map=lambda vj, vk: (vj, vk,), pad_value=None, assume_injective_transform=True)
sch.transform_layout(block=b1, buffer=("write", 0), index_map=lambda vi, vj: (vi, vj,), pad_value=None, assume_injective_transform=True)
sch.transform_block_layout(block=b3, index_map=lambda vi, vj: (vi, vj,))
sch.transform_block_layout(block=b4, index_map=lambda vi, vk: (vi, vk,))
sch.transform_block_layout(block=b5, index_map=lambda vj, vk: (vj, vk,))
sch.transform_block_layout(block=b1, index_map=lambda vi, vj, vk: (vi, vj, vk,))
l6, l7, l8 = sch.get_loops(block=b1)
l9, l10 = sch.split(loop=l8, factors=[None, 16], preserve_unit_iters=True)
l11, l12 = sch.split(loop=l7, factors=[None, 16], preserve_unit_iters=True)
l13, l14 = sch.split(loop=l6, factors=[None, 16], preserve_unit_iters=True)
l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b1)
sch.reorder(l17, l19, l14, l12, l10)
b21 = sch.blockize(loop=l14, preserve_unit_iters=True)
sch.annotate(block_or_loop=b21, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_sync_16x16x16_f16f16f16_trans")
sch.annotate(block_or_loop=b21, ann_key="meta_schedule.auto_tensorize_init", ann_val="wmma_fill_16x16x16_f16")
sch.annotate(block_or_loop=b21, ann_key="warp_execution", ann_val=1)
l22, l23, l24 = sch.get_loops(block=b21)
v25, v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l22, n=5, max_innermost_factor=4, decision=[1, 1, 1, 1, 1])
l30, l31, l32, l33, l34 = sch.split(loop=l22, factors=[v25, v26, v27, v28, v29], preserve_unit_iters=True)
v35, v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l23, n=5, max_innermost_factor=4, decision=[2, 4, 32, 1, 4])
l40, l41, l42, l43, l44 = sch.split(loop=l23, factors=[v35, v36, v37, v38, v39], preserve_unit_iters=True)
v45, v46, v47 = sch.sample_perfect_tile(loop=l24, n=3, max_innermost_factor=4, decision=[256, 1, 4])
l48, l49, l50 = sch.split(loop=l24, factors=[v45, v46, v47], preserve_unit_iters=True)
sch.reorder(l30, l40, l31, l41, l32, l42, l48, l49, l33, l43, l50, l34, l44)
l51 = sch.fuse(l30, l40, preserve_unit_iters=True)
sch.bind(loop=l51, thread_axis="blockIdx.y")
l52 = sch.fuse(l31, l41, preserve_unit_iters=True)
sch.bind(loop=l52, thread_axis="blockIdx.x")
l53 = sch.fuse(l32, l42, preserve_unit_iters=True)
sch.bind(loop=l53, thread_axis="threadIdx.y")
sch.annotate(block_or_loop=b21, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b21, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
sch.transform_layout(block=b21, buffer=("write", 0), index_map=lambda i0, i1: (i0 // 16 // (v28 * v29), i1 // 16 // (v38 * v39), i0 // 16 % (v28 * v29), i1 // 16 % (v38 * v39), i0 % 16, i1 % 16,), pad_value=None, assume_injective_transform=True)
b54 = sch.cache_write(block=b21, write_buffer_index=0, storage_scope="shared.dyn")
sch.reverse_compute_at(block=b54, loop=l52, preserve_unit_loops=True, index=-1)
b55 = sch.cache_write(block=b21, write_buffer_index=0, storage_scope="wmma.accumulator")
l56, l57, l58, l59, l60, l61, l62, l63 = sch.get_loops(block=b54)
sch.reorder(l60, l58, l59, l61)
sch.compute_at(block=b55, loop=l60, preserve_unit_loops=True, index=-1)
l64, l65, l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b55)
l73 = sch.fuse(l67, l68, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.y")
sch.reverse_compute_inline(block=b3)
l74, l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b55)
b82 = sch.blockize(loop=l80, preserve_unit_iters=True)
sch.annotate(block_or_loop=b82, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_store_16x16x16_f16_shared_dyn")
l83, l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b54)
l91 = sch.fuse(l86, l87, l88, l89, l90, preserve_unit_iters=True)
v92 = sch.sample_categorical(candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b54, ann_key="meta_schedule.cooperative_fetch", ann_val=v92)
b93 = sch.cache_read(block=b21, read_buffer_index=0, storage_scope="shared.dyn", consumer_blocks=[b21])
sch.compute_at(block=b93, loop=l48, preserve_unit_loops=True, index=-1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b93)
l100 = sch.fuse(l98, l99, preserve_unit_iters=True)
v101 = sch.sample_categorical(candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b93, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
b102 = sch.cache_read(block=b21, read_buffer_index=1, storage_scope="shared.dyn", consumer_blocks=[b21])
sch.compute_at(block=b102, loop=l48, preserve_unit_loops=True, index=-1)
l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b102)
l109 = sch.fuse(l107, l108, preserve_unit_iters=True)
v110 = sch.sample_categorical(candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b102, ann_key="meta_schedule.cooperative_fetch", ann_val=v110)
b111 = sch.cache_read(block=b21, read_buffer_index=0, storage_scope="wmma.matrix_a")
sch.compute_at(block=b111, loop=l49, preserve_unit_loops=True, index=-1)
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b111)
l119, l120 = sch.split(loop=l118, factors=[None, 16], preserve_unit_iters=True)
l121, l122 = sch.split(loop=l117, factors=[None, 16], preserve_unit_iters=True)
l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b111)
sch.reorder(l130, l122, l120)
b132 = sch.blockize(loop=l122, preserve_unit_iters=True)
sch.annotate(block_or_loop=b132, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_load_16x16x16_f16_a_shared_dyn")
b133 = sch.cache_read(block=b21, read_buffer_index=1, storage_scope="wmma.matrix_b")
sch.compute_at(block=b133, loop=l49, preserve_unit_loops=True, index=-1)
l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b133)
l141, l142 = sch.split(loop=l140, factors=[None, 16], preserve_unit_iters=True)
l143, l144 = sch.split(loop=l139, factors=[None, 16], preserve_unit_iters=True)
l145, l146, l147, l148, l149, l150, l151, l152, l153 = sch.get_loops(block=b133)
sch.reorder(l152, l144, l142)
b154 = sch.blockize(loop=l144, preserve_unit_iters=True)
sch.annotate(block_or_loop=b154, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_load_16x16x16_f16_b_trans_shared_dyn")
sch.compute_inline(block=b4)
sch.compute_inline(block=b5)
sch.storage_align(block=b93, buffer_index=0, axis=-2, factor=32, offset=8)
sch.storage_align(block=b102, buffer_index=0, axis=-2, factor=32, offset=8)
sch.compute_inline(block=b0)
v155 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v155)
2023-05-02 13:37:31 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-02 13:37:31 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2023-05-02 13:37:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 467 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:37:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 927 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:37:38 [INFO] [evolutionary_search.cc:723] Sampled 97 candidate(s)
2023-05-02 13:37:43 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 103 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:37:48 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 81 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:37:52 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 92 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:37:57 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 85 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:37:57 [INFO] [evolutionary_search.cc:649] Scores of the best 16 candidates:
[1 : 16]:	0.9994  0.9984  0.9981  0.9979  0.9969  0.9964  0.9961  0.9943  0.9939  0.9936  0.9913  0.9908  0.9897  0.9897  0.9887  0.9887
2023-05-02 13:37:57 [INFO] [evolutionary_search.cc:727] Got 16 candidate(s) with evolutionary search
2023-05-02 13:37:57 [INFO] [evolutionary_search.cc:730] Sending 16 candidates(s) for measurement
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #1: GFLOPs: 7838.5397. Time: 1643.7886 us. Best GFLOPs: 7838.5397
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #2: GFLOPs: 9789.0422. Time: 1316.2577 us. Best GFLOPs: 9789.0422
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #3: GFLOPs: 16748.9176. Time: 769.2976 us. Best GFLOPs: 16748.9176
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #4: GFLOPs: 11434.6891. Time: 1126.8257 us. Best GFLOPs: 16748.9176
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #5: GFLOPs: 8555.1296. Time: 1506.1025 us. Best GFLOPs: 16748.9176
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #6: GFLOPs: 33671.2952. Time: 382.6672 us. Best GFLOPs: 33671.2952
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #7: GFLOPs: 9786.6457. Time: 1316.5800 us. Best GFLOPs: 33671.2952
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #8: GFLOPs: 34445.6046. Time: 374.0652 us. Best GFLOPs: 34445.6046
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #9: GFLOPs: 8370.0080. Time: 1539.4133 us. Best GFLOPs: 34445.6046
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #10: GFLOPs: 7442.0766. Time: 1731.3584 us. Best GFLOPs: 34445.6046
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #11: GFLOPs: 9558.9656. Time: 1347.9389 us. Best GFLOPs: 34445.6046
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #12: GFLOPs: 4762.3542. Time: 2705.5740 us. Best GFLOPs: 34445.6046
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #13: GFLOPs: 27685.3160. Time: 465.4056 us. Best GFLOPs: 34445.6046
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #14: GFLOPs: 4691.7142. Time: 2746.3100 us. Best GFLOPs: 34445.6046
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #15: GFLOPs: 6354.2357. Time: 2027.7658 us. Best GFLOPs: 34445.6046
2023-05-02 13:39:37 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #16: GFLOPs: 9581.9892. Time: 1344.7001 us. Best GFLOPs: 34445.6046
2023-05-02 13:39:37 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-02 13:39:37 [INFO] [evolutionary_search.cc:715] Picked top 16 candidate(s) from database
2023-05-02 13:39:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 442 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:39:41 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2023-05-02 13:39:45 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 94 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:39:50 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 87 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:39:54 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 60 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:39:59 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 78 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:39:59 [INFO] [evolutionary_search.cc:649] Scores of the best 16 candidates:
[1 : 16]:	0.9998  0.9988  0.9980  0.9977  0.9967  0.9964  0.9961  0.9960  0.9959  0.9953  0.9952  0.9949  0.9930  0.9927  0.9926  0.9918
2023-05-02 13:40:00 [INFO] [evolutionary_search.cc:727] Got 16 candidate(s) with evolutionary search
2023-05-02 13:40:00 [INFO] [evolutionary_search.cc:730] Sending 16 candidates(s) for measurement
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #17: GFLOPs: 16929.4102. Time: 761.0957 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #18: GFLOPs: 9977.8084. Time: 1291.3559 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #19: GFLOPs: 17938.0124. Time: 718.3015 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #20: GFLOPs: 5695.3432. Time: 2262.3574 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #21: GFLOPs: 23767.4012. Time: 542.1250 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #22: GFLOPs: 7521.3952. Time: 1713.1000 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #23: GFLOPs: 11448.2494. Time: 1125.4910 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #24: GFLOPs: 2446.3849. Time: 5266.9152 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #25: GFLOPs: 16745.9570. Time: 769.4336 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #26: GFLOPs: 11121.8577. Time: 1158.5207 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #27: GFLOPs: 11195.3444. Time: 1150.9161 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #28: GFLOPs: 6029.4766. Time: 2136.9851 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #29: GFLOPs: 8452.3098. Time: 1524.4238 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #30: GFLOPs: 7643.7655. Time: 1685.6747 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #31: GFLOPs: 32851.2192. Time: 392.2199 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #32: GFLOPs: 6095.6339. Time: 2113.7919 us. Best GFLOPs: 34445.6046
2023-05-02 13:41:42 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-02 13:41:42 [INFO] [evolutionary_search.cc:715] Picked top 32 candidate(s) from database
2023-05-02 13:41:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 421 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:41:45 [INFO] [evolutionary_search.cc:723] Sampled 59 candidate(s)
2023-05-02 13:41:50 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 110 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:41:55 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 82 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:41:59 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 90 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:42:04 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 83 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:42:04 [INFO] [evolutionary_search.cc:649] Scores of the best 16 candidates:
[1 : 16]:	0.9999  0.9999  0.9989  0.9983  0.9968  0.9950  0.9949  0.9945  0.9944  0.9944  0.9943  0.9926  0.9925  0.9920  0.9920  0.9913
2023-05-02 13:42:05 [INFO] [evolutionary_search.cc:727] Got 16 candidate(s) with evolutionary search
2023-05-02 13:42:05 [INFO] [evolutionary_search.cc:730] Sending 16 candidates(s) for measurement
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #33: GFLOPs: 3943.0137. Time: 3267.7802 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #34: GFLOPs: 31665.2242. Time: 406.9102 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #35: GFLOPs: 3378.2317. Time: 3814.0965 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #36: GFLOPs: 11316.4937. Time: 1138.5949 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #37: GFLOPs: 16866.5265. Time: 763.9333 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #38: GFLOPs: 5700.0446. Time: 2260.4914 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #39: GFLOPs: 18160.1400. Time: 709.5156 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #40: GFLOPs: 12984.9524. Time: 992.2949 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #41: GFLOPs: 1793.4047. Time: 7184.6035 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #42: GFLOPs: 9296.2444. Time: 1386.0330 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #43: GFLOPs: 914.5057. Time: 14089.4717 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #44: GFLOPs: 20554.0407. Time: 626.8793 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #45: GFLOPs: 6425.2295. Time: 2005.3606 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #46: GFLOPs: 7512.9464. Time: 1715.0265 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #47: GFLOPs: 755.4129. Time: 17056.7677 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #48: GFLOPs: 807.2175. Time: 15962.1191 us. Best GFLOPs: 34445.6046
2023-05-02 13:43:45 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-02 13:43:45 [INFO] [evolutionary_search.cc:715] Picked top 48 candidate(s) from database
2023-05-02 13:43:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 411 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:43:48 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2023-05-02 13:43:53 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 86 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:43:57 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 81 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:44:02 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 78 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:44:06 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 71 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:44:06 [INFO] [evolutionary_search.cc:649] Scores of the best 16 candidates:
[1 : 16]:	0.9993  0.9992  0.9991  0.9986  0.9977  0.9975  0.9974  0.9954  0.9935  0.9926  0.9923  0.9921  0.9915  0.9911  0.9908  0.9900
2023-05-02 13:44:07 [INFO] [evolutionary_search.cc:727] Got 16 candidate(s) with evolutionary search
2023-05-02 13:44:07 [INFO] [evolutionary_search.cc:730] Sending 16 candidates(s) for measurement
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #49: GFLOPs: 33782.5620. Time: 381.4069 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #50: GFLOPs: 8008.6223. Time: 1608.8787 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #51: GFLOPs: 4835.9732. Time: 2664.3865 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #52: GFLOPs: 10280.0521. Time: 1253.3888 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #53: GFLOPs: 4258.3463. Time: 3025.7995 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #54: GFLOPs: 3800.2977. Time: 3390.4981 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #55: GFLOPs: 12847.1785. Time: 1002.9363 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #56: GFLOPs: 33874.8921. Time: 380.3673 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #57: GFLOPs: 1810.4210. Time: 7117.0751 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #58: GFLOPs: 8773.5291. Time: 1468.6111 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #59: GFLOPs: 18141.0079. Time: 710.2638 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #60: GFLOPs: 4948.8723. Time: 2603.6036 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #61: GFLOPs: 1658.6031. Time: 7768.5265 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #62: GFLOPs: 8469.9711. Time: 1521.2451 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #63: GFLOPs: 7946.0228. Time: 1621.5536 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #64: GFLOPs: 25983.5454. Time: 495.8870 us. Best GFLOPs: 34445.6046
2023-05-02 13:45:47 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-02 13:45:48 [INFO] [evolutionary_search.cc:715] Picked top 64 candidate(s) from database
2023-05-02 13:45:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 403 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:45:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 817 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:45:54 [INFO] [evolutionary_search.cc:723] Sampled 79 candidate(s)
2023-05-02 13:45:58 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 81 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:46:03 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 92 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:46:08 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 70 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:46:12 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 79 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:46:13 [INFO] [evolutionary_search.cc:649] Scores of the best 16 candidates:
[1 : 16]:	0.9996  0.9987  0.9970  0.9967  0.9953  0.9942  0.9941  0.9924  0.9922  0.9911  0.9910  0.9908  0.9901  0.9895  0.9877  0.9874
2023-05-02 13:46:13 [INFO] [evolutionary_search.cc:727] Got 16 candidate(s) with evolutionary search
2023-05-02 13:46:13 [INFO] [evolutionary_search.cc:730] Sending 16 candidates(s) for measurement
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #65: GFLOPs: 16650.0996. Time: 773.8634 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #66: GFLOPs: 17797.7262. Time: 723.9634 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #67: GFLOPs: 32340.5704. Time: 398.4129 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #68: GFLOPs: 2948.6859. Time: 4369.7098 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #69: GFLOPs: 31208.9139. Time: 412.8597 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #70: GFLOPs: 5915.1980. Time: 2178.2706 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #71: GFLOPs: 9114.8895. Time: 1413.6103 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #72: GFLOPs: 33898.6206. Time: 380.1011 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #73: GFLOPs: 14369.4614. Time: 896.6865 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #74: GFLOPs: 8707.7206. Time: 1479.7101 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #75: GFLOPs: 8832.3592. Time: 1458.8290 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #76: GFLOPs: 7514.0132. Time: 1714.7830 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #77: GFLOPs: 25558.1286. Time: 504.1411 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #78: GFLOPs: 7589.1334. Time: 1697.8094 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #79: GFLOPs: 8839.3792. Time: 1457.6704 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #80: GFLOPs: 13096.5006. Time: 983.8431 us. Best GFLOPs: 34445.6046
2023-05-02 13:47:54 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-02 13:47:55 [INFO] [evolutionary_search.cc:715] Picked top 80 candidate(s) from database
2023-05-02 13:47:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 397 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:48:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 788 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:48:00 [INFO] [evolutionary_search.cc:723] Sampled 76 candidate(s)
2023-05-02 13:48:05 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 97 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:48:09 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 76 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:48:14 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 74 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:48:18 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 76 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:48:19 [INFO] [evolutionary_search.cc:649] Scores of the best 16 candidates:
[1 : 16]:	0.9996  0.9994  0.9994  0.9988  0.9985  0.9982  0.9972  0.9958  0.9953  0.9944  0.9936  0.9921  0.9920  0.9897  0.9886  0.9882
2023-05-02 13:48:19 [INFO] [evolutionary_search.cc:727] Got 16 candidate(s) with evolutionary search
2023-05-02 13:48:19 [INFO] [evolutionary_search.cc:730] Sending 16 candidates(s) for measurement
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #81: GFLOPs: 4628.5330. Time: 2783.7982 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #82: GFLOPs: 16768.8536. Time: 768.3830 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #83: GFLOPs: 24093.9609. Time: 534.7772 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #84: GFLOPs: 20736.4221. Time: 621.3657 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #85: GFLOPs: 9227.7542. Time: 1396.3204 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #86: GFLOPs: 8117.2658. Time: 1587.3451 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #87: GFLOPs: 8127.1019. Time: 1585.4239 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #88: GFLOPs: 9368.7791. Time: 1375.3021 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #89: GFLOPs: 12465.4538. Time: 1033.6488 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #90: GFLOPs: 14999.8030. Time: 859.0047 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #91: GFLOPs: 9493.3426. Time: 1357.2566 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #92: GFLOPs: 1719.7994. Time: 7492.0959 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #93: GFLOPs: 9295.6710. Time: 1386.1185 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #94: GFLOPs: 9033.3075. Time: 1426.3770 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #95: GFLOPs: 6518.8634. Time: 1976.5565 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #96: GFLOPs: 15514.5141. Time: 830.5063 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:01 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-02 13:50:02 [INFO] [evolutionary_search.cc:715] Picked top 96 candidate(s) from database
2023-05-02 13:50:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 376 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:50:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 750 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:50:07 [INFO] [evolutionary_search.cc:723] Sampled 82 candidate(s)
2023-05-02 13:50:12 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 101 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:50:17 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 71 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:50:21 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 86 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:50:25 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f4ef68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x303ba48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x303b958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x303ed68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x3002818)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x2f3b338)]: 63 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x30432b8)]: 0 failure(s)
2023-05-02 13:50:26 [INFO] [evolutionary_search.cc:649] Scores of the best 4 candidates:
[1 : 4]:	0.9989  0.9982  0.9978  0.9974
2023-05-02 13:50:26 [INFO] [evolutionary_search.cc:727] Got 4 candidate(s) with evolutionary search
2023-05-02 13:50:26 [INFO] [evolutionary_search.cc:730] Sending 4 candidates(s) for measurement
2023-05-02 13:50:52 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #97: GFLOPs: 15659.2581. Time: 822.8297 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:52 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #98: GFLOPs: 4731.5717. Time: 2723.1759 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:52 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #99: GFLOPs: 12721.0447. Time: 1012.8808 us. Best GFLOPs: 34445.6046
2023-05-02 13:50:52 [INFO] [task_scheduler.cc:131] [Task #0: main] Trial #100: GFLOPs: 23928.5910. Time: 538.4731 us. Best GFLOPs: 34445.6046
