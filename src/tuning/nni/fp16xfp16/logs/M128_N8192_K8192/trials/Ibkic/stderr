[12:32:57] /workspace/v-leiwang3/lowbit_tvm/src/target/source/codegen_cuda.cc:1077: wait_cnt: 2
[12:32:57] /workspace/v-leiwang3/lowbit_tvm/src/target/source/codegen_cuda.cc:1077: wait_cnt: 1
[12:32:57] /workspace/v-leiwang3/lowbit_tvm/src/target/source/codegen_cuda.cc:1077: wait_cnt: 0
Traceback (most recent call last):
  File "./fp16xfp16_nt_padwmma_template.py", line 242, in <module>
    cuda_mod = tvm.build(sch.mod, target="cuda")
  File "/workspace/v-leiwang3/lowbit_tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/workspace/v-leiwang3/lowbit_tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  6: TVMFuncCall
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)
  4: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  3: tvm::codegen::Build(tvm::IRModule, tvm::Target)
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  1: tvm::codegen::BuildCUDA(tvm::IRModule, tvm::Target)
  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .cold]
  File "/workspace/v-leiwang3/lowbit_tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/workspace/v-leiwang3/lowbit_tvm/python/tvm/contrib/nvcc.py", line 190, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/workspace/v-leiwang3/lowbit_tvm/python/tvm/contrib/nvcc.py", line 114, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
#include <cuda_fp16.h>
__device__ half max(half a, half b)
{
  return __hgt(__half(a), __half(b)) ? a : b;
}
__device__ half min(half a, half b)
{
  return __hlt(__half(a), __half(b)) ? a : b;
}
#else

typedef unsigned short uint16_t;
typedef unsigned char uint8_t;
typedef signed char int8_t;
typedef int int32_t;
typedef unsigned long long uint64_t;
typedef unsigned int uint32_t;

#define TVM_FORCE_INLINE inline __attribute__((always_inline))
#define TVM_XINLINE TVM_FORCE_INLINE __device__ __host__
#define TVM_ALIGNED(x) __attribute__ ((aligned(x)))
#define TVM_HALF_OPERATOR(RTYPE, OP)                              \
  TVM_XINLINE RTYPE operator OP (half a, half b) {                \
    return RTYPE(float(a) OP float(b));                           \
  }                                                               \
  template<typename T>                                            \
  TVM_XINLINE RTYPE operator OP (half a, T b) {                   \
    return RTYPE(float(a) OP float(b));                           \
  }                                                               \
  template<typename T>                                            \
  TVM_XINLINE RTYPE operator OP (T a, half b) {                   \
    return RTYPE(float(a) OP float(b));                           \
  }

#define TVM_HALF_ASSIGNOP(AOP, OP)                                \
  template<typename T>                                            \
  TVM_XINLINE half operator AOP (const T& a) {                    \
    return *this = half(float(*this) OP float(a));                \
  }                                                               \
  template<typename T>                                            \
  TVM_XINLINE half operator AOP (const volatile T& a) volatile {  \
    return *this = half(float(*this) OP float(a));                \
  }

class TVM_ALIGNED(2) half {
 public:
  uint16_t half_;

  static TVM_XINLINE half Binary(uint16_t value) {
    half res;
    res.half_ = value;
    return res;
  }

  TVM_XINLINE half() {}

  TVM_XINLINE half(const float& value) { constructor(value); }
  TVM_XINLINE explicit half(const double& value) { constructor(value); }
  TVM_XINLINE explicit half(const int8_t& value) { constructor(value); }
  TVM_XINLINE explicit half(const uint8_t& value) { constructor(value); }
  TVM_XINLINE explicit half(const int32_t& value) { constructor(value); }
  TVM_XINLINE explicit half(const uint32_t& value) { constructor(value); }
  TVM_XINLINE explicit half(const long long& value) { constructor(value); }
  TVM_XINLINE explicit half(const uint64_t& value) { constructor(value); }

  TVM_XINLINE operator float() const {                          \
    return float(half2float(half_));                            \
  }                                                             \
  TVM_XINLINE operator float() const volatile {                 \
    return float(half2float(half_));                            \
  }


  TVM_HALF_ASSIGNOP(+=, +)
  TVM_HALF_ASSIGNOP(-=, -)
  TVM_HALF_ASSIGNOP(*=, *)
  TVM_HALF_ASSIGNOP(/=, /)

  TVM_XINLINE half operator+() {
    return *this;
  }

  TVM_XINLINE half operator-() {
    return half(-float(*this));
  }

  TVM_XINLINE half operator=(const half& a) {
    half_ = a.half_;
    return a;
  }

  template<typename T>
  TVM_XINLINE half operator=(const T& a) {
    return *this = half(a);
  }

  TVM_XINLINE half operator=(const half& a) volatile {
    half_ = a.half_;
    return a;
  }

  template<typename T>
  TVM_XINLINE half operator=(const T& a) volatile {
    return *this = half(a);
  }

 private:
  union Bits {
    float f;
    int32_t si;
    uint32_t ui;
  };

  static int const fp16FractionBits = 10;
  static int const fp32FractionBits = 23;
  static int32_t const fp32FractionMask = ~(~0u << fp32FractionBits);   // == 0x7fffff
  static int32_t const fp32HiddenBit = 1 << fp32FractionBits;   // == 0x800000
  static int const shift = fp32FractionBits - fp16FractionBits;   // == 13
  static int const shiftSign = 16;
  static int32_t const expAdjust = 127 - 15;   // exp32-127 = exp16-15, so exp16 = exp32 - (127-15)

  static int32_t const infN = 0x7F800000;   // flt32 infinity
  static int32_t const maxN = 0x477FFFFF;   // max flt32 that's a flt16 normal after >> by shift
  static int32_t const minN = 0x38800000;   // min flt16 normal as a flt32
  static int32_t const maxZ = 0x33000000;   // max fp32 number that's still rounded to zero in fp16
  static int32_t const signN = 0x80000000;  // flt32 sign bit

  static int32_t const infC = infN >> shift;
  static int32_t const nanN = (infC + 1) << shift;   // minimum flt16 nan as a flt32
  static int32_t const maxC = maxN >> shift;
  static int32_t const minC = minN >> shift;
  static int32_t const signC = signN >> shiftSign;  // flt16 sign bit

  static int32_t const mulN = 0x52000000;  // (1 << 23) / minN
  static int32_t const mulC = 0x33800000;  // minN / (1 << (23 - shift))

  static int32_t const subC = 0x003FF;  // max flt32 subnormal down shifted
  static int32_t const norC = 0x00400;  // min flt32 normal down shifted

  static int32_t const maxD = infC - maxC - 1;
  static int32_t const minD = minC - subC - 1;

  TVM_XINLINE uint16_t float2half(const float& value) const {
    Bits v;
    v.f = value;
    uint32_t sign = v.si & signN;    // grab sign bit
    v.si ^= sign;                    // clear sign bit from v
    sign >>= shiftSign;              // logical shift sign to fp16 position

    if (v.si <= maxZ) {
      // Handle eventual zeros here to ensure
      // vshift will not exceed 32 below.
      v.ui = 0;
    } else if (v.si < minN) {
      // Handle denorms
      uint32_t exp32 = v.ui >> fp32FractionBits;
      int32_t exp16 = exp32 - expAdjust;
      // If exp16 == 0 (just into the denorm range), then significant should be shifted right 1.
      // Smaller (so negative) exp16 values should result in greater right shifts.
      uint32_t vshift = 1 - exp16;
      uint32_t significand = fp32HiddenBit | (v.ui & fp32FractionMask);
      v.ui = significand >> vshift;
      v.ui += (v.ui & 0x3fff) != 0x1000 || (significand & 0x7ff) ? 0x1000 : 0;
    } else if (v.si <= maxN) {
      // Handle norms
      v.ui += (v.ui & 0x3fff) != 0x1000 ? 0x1000 : 0;
      v.ui -= expAdjust << fp32FractionBits;
    } else if (v.si <= infN) {
      v.si = infN;
    } else if (v.si < nanN) {
      v.si = nanN;
    }

    v.ui >>= shift;
    return sign | (v.ui & 0x7fff);
  }

  // Same as above routine, except for addition of volatile keyword
  TVM_XINLINE uint16_t float2half(
    const volatile float& value) const volatile {
    Bits v;
    v.f = value;
    uint32_t sign = v.si & signN;    // grab sign bit
    v.si ^= sign;                    // clear sign bit from v
    sign >>= shiftSign;              // logical shift sign to fp16 position

    if (v.si <= maxZ) {
      // Handle eventual zeros here to ensure
      // vshift will not exceed 32 below.
      v.ui = 0;
    } else if (v.si < minN) {
      // Handle denorms
      uint32_t exp32 = v.ui >> fp32FractionBits;
      int32_t exp16 = exp32 - expAdjust;
      // If exp16 == 0 (just into the denorm range), then significant should be shifted right 1.
      // Smaller (so negative) exp16 values should result in greater right shifts.
      uint32_t vshift = 1 - exp16;
      uint32_t significand = fp32HiddenBit | (v.ui & fp32FractionMask);
      v.ui = significand >> vshift;
      v.ui += (v.ui & 0x3fff) != 0x1000 || (significand & 0x7ff) ? 0x1000 : 0;
    } else if (v.si <= maxN) {
      // Handle norms
      v.ui += (v.ui & 0x3fff) != 0x1000 ? 0x1000 : 0;
      v.ui -= expAdjust << fp32FractionBits;
    } else if (v.si <= infN) {
      v.si = infN;
    } else if (v.si < nanN) {
      v.si = nanN;
    }

    v.ui >>= shift;
    return sign | (v.ui & 0x7fff);
  }

  TVM_XINLINE float half2float(const uint16_t& value) const {
    Bits v;
    v.ui = value;
    int32_t sign = v.si & signC;
    v.si ^= sign;
    sign <<= shiftSign;
    v.si ^= ((v.si + minD) ^ v.si) & -(v.si > subC);
    v.si ^= ((v.si + maxD) ^ v.si) & -(v.si > maxC);
    Bits s;
    s.si = mulC;
    s.f *= v.si;
    int32_t mask = -(norC > v.si);
    v.si <<= shift;
    v.si ^= (s.si ^ v.si) & mask;
    v.si |= sign;
    return v.f;
  }

  TVM_XINLINE float half2float(
    const volatile uint16_t& value) const volatile {
    Bits v;
    v.ui = value;
    int32_t sign = v.si & signC;
    v.si ^= sign;
    sign <<= shiftSign;
    v.si ^= ((v.si + minD) ^ v.si) & -(v.si > subC);
    v.si ^= ((v.si + maxD) ^ v.si) & -(v.si > maxC);
    Bits s;
    s.si = mulC;
    s.f *= v.si;
    int32_t mask = -(norC > v.si);
    v.si <<= shift;
    v.si ^= (s.si ^ v.si) & mask;
    v.si |= sign;
    return v.f;
  }

  template<typename T>
  TVM_XINLINE void constructor(const T& value) {
    half_ = float2half(float(value));
  }
};

TVM_HALF_OPERATOR(half, +)
TVM_HALF_OPERATOR(half, -)
TVM_HALF_OPERATOR(half, *)
TVM_HALF_OPERATOR(half, /)
TVM_HALF_OPERATOR(bool, >)
TVM_HALF_OPERATOR(bool, <)
TVM_HALF_OPERATOR(bool, >=)
TVM_HALF_OPERATOR(bool, <=)

TVM_XINLINE half __float2half_rn(const float a) {
  return half(a);
}
#endif


// Pack two half values.
static inline __device__ __host__ unsigned
__pack_half2(const half x, const half y) {
  unsigned v0 = *((unsigned short *)&x);
  unsigned v1 = *((unsigned short *)&y);
  return (v1 << 16) | v0;
}

// Some fp16 math functions are not supported in cuda_fp16.h,
// so we define them here to make sure the generated CUDA code
// is valid.
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
#define CUDA_UNSUPPORTED_HALF_MATH_BINARY(HALF_MATH_NAME, FP32_MATH_NAME) \
static inline __device__ __host__ half HALF_MATH_NAME(half x, half y) {   \
  float tmp_x = __half2float(x);                                          \
  float tmp_y = __half2float(y);                                          \
  float result = FP32_MATH_NAME(tmp_x, tmp_y);                            \
  return __float2half(result);                                            \
}

#define CUDA_UNSUPPORTED_HALF_MATH_UNARY(HALF_MATH_NAME, FP32_MATH_NAME) \
static inline __device__ __host__ half HALF_MATH_NAME(half x) {          \
  float tmp_x = __half2float(x);                                         \
  float result = FP32_MATH_NAME(tmp_x);                                  \
  return __float2half(result);                                           \
}

CUDA_UNSUPPORTED_HALF_MATH_BINARY(hpow, powf)
CUDA_UNSUPPORTED_HALF_MATH_UNARY(htanh, tanhf)
CUDA_UNSUPPORTED_HALF_MATH_UNARY(htan, tanf)
CUDA_UNSUPPORTED_HALF_MATH_UNARY(hatan, atanf)
CUDA_UNSUPPORTED_HALF_MATH_UNARY(herf, erf)

#undef CUDA_UNSUPPORTED_HALF_MATH_BINARY
#undef CUDA_UNSUPPORTED_HALF_MATH_UNARY

#endif
#include <mma.h>

#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \
     (__CUDACC_VER_MAJOR__ > 11))
#define TVM_ENABLE_L2_PREFETCH 1
#else
#define TVM_ENABLE_L2_PREFETCH 0
#endif

#if (__CUDACC_VER_MAJOR__ >= 11) 
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 1
#else
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 0
#endif

#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) main_kernel0(half* __restrict__ A, half* __restrict__ B, half* __restrict__ C) {
  nvcuda::wmma::fragment<nvcuda::wmma::accumulator, 16, 16, 16, half> C_wmma_accumulator[4];
  __shared__ half A_shared[15360];
  __shared__ half B_shared[30720];
  nvcuda::wmma::fragment<nvcuda::wmma::matrix_a, 16, 16, 16, half, nvcuda::wmma::row_major> A_shared_wmma_matrix_a[1];
  nvcuda::wmma::fragment<nvcuda::wmma::matrix_b, 16, 16, 16, half, nvcuda::wmma::col_major> B_shared_wmma_matrix_b[4];
  for (int j_0_2_init = 0; j_0_2_init < 4; ++j_0_2_init) {
    nvcuda::wmma::fill_fragment(C_wmma_accumulator[j_0_2_init], 0.000000e+00f);
  }
  for (int ax0_ax1_fused_4_s = 0; ax0_ax1_fused_4_s < 8; ++ax0_ax1_fused_4_s) {
    if (((((((int)threadIdx.y) * 1024) + (((int)threadIdx.z) * 256)) + (((int)threadIdx.x) * 8)) + ax0_ax1_fused_4_s) < 4096) {
      A_shared[(((((((int)threadIdx.y) * 1280) + (((int)threadIdx.z) * 320)) + ((((int)threadIdx.x) >> 2) * 40)) + ((((int)threadIdx.x) & 3) * 8)) + ax0_ax1_fused_4_s)] = A[(((((((int)threadIdx.y) * 262144) + (((int)threadIdx.z) * 65536)) + ((((int)threadIdx.x) >> 2) * 8192)) + ((((int)threadIdx.x) & 3) * 8)) + ax0_ax1_fused_4_s)];
    }
  }

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(B_shared + ((((((int)threadIdx.y) * 1280) + (((int)threadIdx.z) * 320)) + ((((int)threadIdx.x) >> 2) * 40)) + ((((int)threadIdx.x) & 3) * 8)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(B_shared + ((((((int)threadIdx.y) * 1280) + (((int)threadIdx.z) * 320)) + ((((int)threadIdx.x) >> 2) * 40)) + ((((int)threadIdx.x) & 3) * 8))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((int)blockIdx.x) * 16777216) + (((int)blockIdx.z) * 2097152)) + (((int)threadIdx.y) * 262144)) + (((int)threadIdx.z) * 65536)) + ((((int)threadIdx.x) >> 2) * 8192)) + ((((int)threadIdx.x) & 3) * 8)))), "n"(16)
    );
  }
__asm__ __volatile__("cp.async.commit_group;");

  for (int ax0_ax1_fused_4_s_1 = 0; ax0_ax1_fused_4_s_1 < 8; ++ax0_ax1_fused_4_s_1) {
    if (((((((int)threadIdx.y) * 1024) + (((int)threadIdx.z) * 256)) + (((int)threadIdx.x) * 8)) + ax0_ax1_fused_4_s_1) < 4096) {
      A_shared[((((((((int)threadIdx.y) * 1280) + (((int)threadIdx.z) * 320)) + ((((int)threadIdx.x) >> 2) * 40)) + ((((int)threadIdx.x) & 3) * 8)) + ax0_ax1_fused_4_s_1) + 5120)] = A[((((((((int)threadIdx.y) * 262144) + (((int)threadIdx.z) * 65536)) + ((((int)threadIdx.x) >> 2) * 8192)) + ((((int)threadIdx.x) & 3) * 8)) + ax0_ax1_fused_4_s_1) + 32)];
    }
  }

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(B_shared + (((((((int)threadIdx.y) * 1280) + (((int)threadIdx.z) * 320)) + ((((int)threadIdx.x) >> 2) * 40)) + ((((int)threadIdx.x) & 3) * 8)) + 10240))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(B_shared + (((((((int)threadIdx.y) * 1280) + (((int)threadIdx.z) * 320)) + ((((int)threadIdx.x) >> 2) * 40)) + ((((int)threadIdx.x) & 3) * 8)) + 10240)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + (((((((((int)blockIdx.x) * 16777216) + (((int)blockIdx.z) * 2097152)) + (((int)threadIdx.y) * 262144)) + (((int)threadIdx.z) * 65536)) + ((((int)threadIdx.x) >> 2) * 8192)) + ((((int)threadIdx.x) & 3) * 8)) + 32))), "n"(16)
    );
  }
__asm__ __volatile__("cp.async.commit_group;");

  for (int k_0_0 = 0; k_0_0 < 254; ++k_0_0) {
    __syncthreads();
    for (int ax0_ax1_fused_4_s_2 = 0; ax0_ax1_fused_4_s_2 < 8; ++ax0_ax1_fused_4_s_2) {
      if (((((((int)threadIdx.y) * 1024) + (((int)threadIdx.z) * 256)) + (((int)threadIdx.x) * 8)) + ax0_ax1_fused_4_s_2) < 4096) {
        A_shared[((((((((k_0_0 + 2) % 3) * 5120) + (((int)threadIdx.y) * 1280)) + (((int)threadIdx.z) * 320)) + ((((int)threadIdx.x) >> 2) * 40)) + ((((int)threadIdx.x) & 3) * 8)) + ax0_ax1_fused_4_s_2)] = A[(((((((((int)threadIdx.y) * 262144) + (((int)threadIdx.z) * 65536)) + ((((int)threadIdx.x) >> 2) * 8192)) + (k_0_0 * 32)) + ((((int)threadIdx.x) & 3) * 8)) + ax0_ax1_fused_4_s_2) + 64)];
      }
    }

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(B_shared + (((((((k_0_0 + 2) % 3) * 10240) + (((int)threadIdx.y) * 1280)) + (((int)threadIdx.z) * 320)) + ((((int)threadIdx.x) >> 2) * 40)) + ((((int)threadIdx.x) & 3) * 8)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(B_shared + (((((((k_0_0 + 2) % 3) * 10240) + (((int)threadIdx.y) * 1280)) + (((int)threadIdx.z) * 320)) + ((((int)threadIdx.x) >> 2) * 40)) + ((((int)threadIdx.x) & 3) * 8))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int)blockIdx.x) * 16777216) + (((int)blockIdx.z) * 2097152)) + (((int)threadIdx.y) * 262144)) + (((int)threadIdx.z) * 65536)) + ((((int)threadIdx.x) >> 2) * 8192)) + (k_0_0 * 32)) + ((((int)threadIdx.x) & 3) * 8)) + 64))), "n"(16)
    );
  }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 2;");

    __syncthreads();
    for (int k_0_1 = 0; k_0_1 < 2; ++k_0_1) {
      nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[0], (&(A_shared[((((k_0_0 % 3) * 5120) + (((int)threadIdx.y) * 640)) + (k_0_1 * 16))])), 40);
      for (int ax0_0 = 0; ax0_0 < 4; ++ax0_0) {
        nvcuda::wmma::load_matrix_sync(B_shared_wmma_matrix_b[ax0_0], (&(B_shared[(((((k_0_0 % 3) * 10240) + (((int)threadIdx.z) * 2560)) + (ax0_0 * 640)) + (k_0_1 * 16))])), 40);
      }
      for (int j_0_2 = 0; j_0_2 < 4; ++j_0_2) {
        nvcuda::wmma::mma_sync(C_wmma_accumulator[j_0_2], A_shared_wmma_matrix_a[0], B_shared_wmma_matrix_b[j_0_2], C_wmma_accumulator[j_0_2]);
      }
    }
  }
__asm__ __volatile__("cp.async.wait_group 1;");

  __syncthreads();
  for (int k_0_1_1 = 0; k_0_1_1 < 2; ++k_0_1_1) {
    nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[0], (&(A_shared[(((((int)threadIdx.y) * 640) + (k_0_1_1 * 16)) + 10240)])), 40);
    for (int ax0_0_1 = 0; ax0_0_1 < 4; ++ax0_0_1) {
      nvcuda::wmma::load_matrix_sync(B_shared_wmma_matrix_b[ax0_0_1], (&(B_shared[((((((int)threadIdx.z) * 2560) + (ax0_0_1 * 640)) + (k_0_1_1 * 16)) + 20480)])), 40);
    }
    for (int j_0_2_1 = 0; j_0_2_1 < 4; ++j_0_2_1) {
      nvcuda::wmma::mma_sync(C_wmma_accumulator[j_0_2_1], A_shared_wmma_matrix_a[0], B_shared_wmma_matrix_b[j_0_2_1], C_wmma_accumulator[j_0_2_1]);
    }
  }
__asm__ __volatile__("cp.async.wait_group 0;");

  __syncthreads();
  for (int k_0_1_2 = 0; k_0_1_2 < 2; ++k_0_1_2) {
    nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[0], (&(A_shared[((((int)threadIdx.y) * 640) + (k_0_1_2 * 16))])), 40);
    for (int ax0_0_2 = 0; ax0_0_2 < 4; ++ax0_0_2) {
      nvcuda::wmma::load_matrix_sync(B_shared_wmma_matrix_b[ax0_0_2], (&(B_shared[(((((int)threadIdx.z) * 2560) + (ax0_0_2 * 640)) + (k_0_1_2 * 16))])), 40);
    }
    for (int j_0_2_2 = 0; j_0_2_2 < 4; ++j_0_2_2) {
      nvcuda::wmma::mma_sync(C_wmma_accumulator[j_0_2_2], A_shared_wmma_matrix_a[0], B_shared_wmma_matrix_b[j_0_2_2], C_wmma_accumulator[j_0_2_2]);
    }
  }
  for (int ax1_0 = 0; ax1_0 < 4; ++ax1_0) {
    nvcuda::wmma::store_matrix_sync((&(C[(((((((int)threadIdx.y) * 131072) + (((int)blockIdx.x) * 2048)) + (((int)blockIdx.z) * 256)) + (((int)threadIdx.z) * 64)) + (ax1_0 * 16))])), C_wmma_accumulator[ax1_0], 8192, nvcuda::wmma::mem_row_major);
  }
}


Compilation error:
/tmp/tvm-debug-mode-tempdirs/2023-05-03T12-32-57___qs13bvud/00000/my_kernel.cu(276): warning #177-D: function "__pack_half2" was declared but never referenced

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/tmp/tvm-debug-mode-tempdirs/2023-05-03T12-32-57___qs13bvud/00000/my_kernel.cu(301): warning #177-D: function "hpow" was declared but never referenced

/tmp/tvm-debug-mode-tempdirs/2023-05-03T12-32-57___qs13bvud/00000/my_kernel.cu(302): warning #177-D: function "htanh" was declared but never referenced

/tmp/tvm-debug-mode-tempdirs/2023-05-03T12-32-57___qs13bvud/00000/my_kernel.cu(303): warning #177-D: function "htan" was declared but never referenced

/tmp/tvm-debug-mode-tempdirs/2023-05-03T12-32-57___qs13bvud/00000/my_kernel.cu(304): warning #177-D: function "hatan" was declared but never referenced

/tmp/tvm-debug-mode-tempdirs/2023-05-03T12-32-57___qs13bvud/00000/my_kernel.cu(305): warning #177-D: function "herf" was declared but never referenced

ptxas error   : Entry function 'main_kernel0' uses too much shared data (0x16800 bytes, 0xc000 max)

